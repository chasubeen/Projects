{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90aa61b5",
   "metadata": {},
   "source": [
    "# **1. 데이터 차원**\n",
    "- 데이터 자채는 충분함\n",
    "- 불균형 데이터 -> Data Augmentation/ One-shot Learning\n",
    "- 데이터 범위(scale) 조정: 정규화/규제화/표준화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c9b54",
   "metadata": {},
   "source": [
    "# **2. 알고리즘 튜닝**\n",
    "- 진단\n",
    "    - train/ valid 모두 accuracy가 그다지 높지 않음 -> 과소적합 \n",
    "    - 과적합 방지를 위한 기법들 도입하기(배치 정규화, 드롭아웃, 조기 종료)\n",
    "- 가중치\n",
    "    - AutoEncoder 등과 같은 비지도 학습을 통해 사전 훈련\n",
    "    - 이후 다시 지도 학습 진행\n",
    "- 학습률(learning rate)\n",
    "    - 현재 네트워크의 계층이 많음 -> 높게 설정할 필요가 있음..?\n",
    "- 활성화 함수/손실함수\n",
    "    - 활성화 함수: softmax\n",
    "    - 손실 함수: CrossEntropyLoss\n",
    "    - 활성화 함수 변경 시 손실 함수도 함께 변경해야 하는 경우가 많음 -> 주의하기\n",
    "- 배치/ Epoch\n",
    "    - DataLoader에서 batch size 조정(batch_size = 32 or 64) --- ?\n",
    "    - Epoch: 과소적합의 경향성 -> 증가시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89069eb",
   "metadata": {},
   "source": [
    "# **3. 하드웨어 차원**\n",
    "- 역전파처럼 복잡한 미적분 연산의 경우 병렬 연산을 해야 속도/ 정확도 증가\n",
    "- 장치를 cpu에서 gpu로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "511168c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 필요한 라이브러리 준비\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc18c93",
   "metadata": {},
   "source": [
    "### **DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5d8b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = './DataSet/archive/train/'\n",
    "train_data_df = pd.read_csv(os.path.join(train_data_dir,'train.csv'))\n",
    "\n",
    "# 분류에 사용할 class 정의(7개의 감정들)\n",
    "feelings_list = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "965a7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 48\n",
    "\n",
    "### 이미지 파일 경로를 리스트 형태로 저장하기 위한 함수\n",
    "def list_image_file(data_dir,sub_dir):\n",
    "    image_files = []\n",
    "    \n",
    "    images_dir = os.path.join(data_dir,sub_dir)\n",
    "    for file_path in os.listdir(images_dir):\n",
    "        image_files.append(os.path.join(sub_dir,file_path))\n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc9ba21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습 데이터셋 클래스\n",
    "class Feeling_dataset(Dataset):\n",
    "    ### 생성자\n",
    "    def __init__(self,data_dir,transform = None):\n",
    "        self.data_dir = data_dir # 데이터가 저장된 상위 directory\n",
    "        angry_imgs = list_image_file(data_dir,'angry')\n",
    "        disgust_imgs = list_image_file(data_dir,'disgust')\n",
    "        fear_imgs = list_image_file(data_dir,'fear')\n",
    "        happy_imgs = list_image_file(data_dir,'happy')\n",
    "        neutral_imgs = list_image_file(data_dir,'neutral')\n",
    "        sad_imgs = list_image_file(data_dir,'sad')\n",
    "        surprise_imgs = list_image_file(data_dir,'surprise')\n",
    "        \n",
    "        # 모든 사진들의 경로를 하나의 리스트에 저장\n",
    "        self.files_path = angry_imgs + disgust_imgs + fear_imgs + happy_imgs + neutral_imgs + sad_imgs + surprise_imgs\n",
    "        self.transform = transform\n",
    "    \n",
    "    ### 데이터 개수 확인\n",
    "    def __len__(self):\n",
    "        return len(self.files_path) # 전체 데이터 개수\n",
    "    \n",
    "    ### getitem\n",
    "    def __getitem__(self,index):\n",
    "        # image(feature data)\n",
    "        image_file = os.path.join(self.data_dir,self.files_path[index])\n",
    "        image = cv2.imread(image_file)\n",
    "        image = cv2.resize(image,dsize = (IMAGE_SIZE,IMAGE_SIZE),interpolation = cv2.INTER_LINEAR)\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # feeling(target data)\n",
    "        target = feelings_list.index(self.files_path[index].split(os.sep)[0])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image) # feature data에 대해서만 데이터 변형 수행\n",
    "            target = torch.Tensor([target]).long()\n",
    "            \n",
    "        return {'image':image,'target':target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f3fde09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformation\n",
    "\n",
    "# 학습 feature data 변환\n",
    "train_transformer = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), # 수평으로 뒤집기\n",
    "    transforms.RandomVerticalFlip(), # 수직으로 뒤집기\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]), # 정규화\n",
    "    transforms.ToTensor() # 텐서로 변환\n",
    "])\n",
    "\n",
    "# 검증 feature data 변환\n",
    "val_transformer = transforms.Compose([\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]), # 정규화\n",
    "    transforms.ToTensor() # 텐서로 변환\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c087452",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터로더 구현\n",
    "def build_dataloader(train_data_dir,val_data_dir):\n",
    "    dataloaders = {}\n",
    "    train_dset = Feeling_dataset(train_data_dir,train_transformer)\n",
    "    dataloaders['train'] = DataLoader(train_dset,batch_size = 64,shuffle = True,drop_last = True)\n",
    "    \n",
    "    val_dset = Feeling_dataset(val_data_dir,val_transformer)\n",
    "    dataloaders['val'] = DataLoader(val_dset,batch_size = 16,shuffle = False,drop_last = False)\n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f83ad3",
   "metadata": {},
   "source": [
    "- batch size 조정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30ce035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = './DataSet/archive/train/'\n",
    "val_data_dir = train_data_dir = './DataSet/archive/valid/'\n",
    "dataloaders = build_dataloader(train_data_dir,val_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd26bd",
   "metadata": {},
   "source": [
    "### **Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b7ae64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델 생성 함수\n",
    "# 기존의 VGG19 모델 호출 -> head 부분 수정\n",
    "def build_vgg19_based_model(device_name = 'gpu'):\n",
    "    device = torch.device(device_name)\n",
    "    model = models.vgg19(pretrained = True) # 이미 학습된 vgg19 모델 불러오기\n",
    "    # 일반 NN Layer(FC Layer)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(output_size = (1,1)) # 각 구역의 평균값 출력\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Flatten(), # 평탄화\n",
    "        nn.Linear(512,256), # 512 -> 256\n",
    "        nn.ReLU(), # 활성화 함수\n",
    "        nn.Dropout(0.1), # 과적합 방지\n",
    "        nn.Linear(256,7), # 256 -> 7(7개의 감정으로 분류되니)\n",
    "        nn.Softmax() # 활성화 함수(각 클래스에 속할 확률 추정)\n",
    "    )\n",
    "    \n",
    "    return model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
