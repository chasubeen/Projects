{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ebaad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 필요한 라이브러리 준비\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14eff388",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = './DataSet/archive/test'\n",
    "\n",
    "# 분류에 사용할 class 정의(7개의 감정들)\n",
    "feelings_list = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b202728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 48\n",
    "\n",
    "### 이미지 파일 경로를 리스트 형태로 저장하기 위한 함수\n",
    "def list_image_file(data_dir,sub_dir):\n",
    "    image_files = []\n",
    "    \n",
    "    images_dir = os.path.join(data_dir,sub_dir)\n",
    "    for file_path in os.listdir(images_dir):\n",
    "        image_files.append(os.path.join(sub_dir,file_path))\n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d145b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_angry_imgs = list_image_file(test_data_dir,'angry')\n",
    "test_disgust_imgs = list_image_file(test_data_dir,'disgust')\n",
    "test_fear_imgs = list_image_file(test_data_dir,'fear')\n",
    "test_happy_imgs = list_image_file(test_data_dir,'happy')\n",
    "test_neutral_imgs = list_image_file(test_data_dir,'neutral')\n",
    "test_sad_imgs = list_image_file(test_data_dir,'sad')\n",
    "test_surprise_imgs = list_image_file(test_data_dir,'surprise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fdd2179",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9be3df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 테스트 이미지 전처리\n",
    "\n",
    "def preprocess_image(image):\n",
    "    transformer = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    tensor_image = transformer(image) # image: (C,H,W)\n",
    "    tensor_image = tensor_image.unsqueeze(0) # (B(batch),C,H,W)\n",
    "    \n",
    "    return tensor_image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54aed5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 예측을 위한 함수\n",
    "\n",
    "def model_predict(image,model):\n",
    "    tensor_image = preprocess_image(image) # 이미지(feature data)\n",
    "    prediction = model(tensor_image) # 예측\n",
    "    \n",
    "    _, pred_label1 = torch.max(prediction.detach(),dim = 1) # dim = 1 : 1차원으로 이미지를 가져오겠다.\n",
    "    print('pred_label1: ',pred_label1)\n",
    "    \n",
    "    pred_label = pred_label1.squeeze(0) # 차원 증가\n",
    "    print('pred_label2: ',pred_label)\n",
    "    \n",
    "    return pred_label.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61199fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델 생성 함수\n",
    "# 기존의 VGG19 모델 호출 -> head 부분 수정\n",
    "\n",
    "def build_vgg19_based_model():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = models.vgg19(pretrained = True) # 이미 학습된 vgg19 모델 불러오기\n",
    "    # 일반 NN Layer(FC Layer)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(output_size = (1,1)) # 각 구역의 평균값 출력\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Flatten(), # 평탄화\n",
    "        nn.Linear(512,256), # 512 -> 256\n",
    "        nn.ReLU(), # 활성화 함수\n",
    "        nn.Dropout(0.1), # 과적합 방지\n",
    "        nn.Linear(256,7), # 256 -> 7(7개의 감정으로 분류되니)\n",
    "        nn.Softmax() # 활성화 함수(각 클래스에 속할 확률 추정)\n",
    "    )\n",
    "    \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24d96c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\users\\bin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=256, out_features=7, bias=True)\n",
       "    (5): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 학습된 모델 불러오기\n",
    "\n",
    "ckpt = torch.load('./best_model/model_57.pth')\n",
    "\n",
    "model = build_vgg19_based_model()\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e331fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 이미지 파일을 RGB 3차원 배열로 가져오는 함수\n",
    "\n",
    "def get_RGB_image(data_dir,file_name):\n",
    "    image_file = os.path.join(data_dir,file_name) # 이미지 경로 설정\n",
    "    image = cv2.imread(image_file) # 이미지 열기\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB) # BGR -> RGB\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "424e74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 개수가 적은 감정에 개수 맞추기\n",
    "\n",
    "min_num_files = min(len(test_angry_imgs), len(test_disgust_imgs), len(test_fear_imgs),len(test_happy_imgs),\n",
    "                    len(test_neutral_imgs),len(test_sad_imgs),len(test_surprise_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a9dd58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031de2bf445b4df4afacbcf45bec58aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=109), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 예측 결과 시각화\n",
    "\n",
    "@interact(index = (0, min_num_files - 1))\n",
    "def show_result(index = 0):\n",
    "    # 테스트 이미지 파일 가져오기\n",
    "    angry_image = get_RGB_image(test_data_dir, test_angry_imgs[index])\n",
    "    disgust_image = get_RGB_image(test_data_dir, test_disgust_imgs[index])\n",
    "    fear_image = get_RGB_image(test_data_dir, test_fear_imgs[index])\n",
    "    happy_image = get_RGB_image(test_data_dir, test_happy_imgs[index])\n",
    "    neutral_image = get_RGB_image(test_data_dir, test_neutral_imgs[index])\n",
    "    sad_image = get_RGB_image(test_data_dir, test_sad_imgs[index])\n",
    "    surprise_image = get_RGB_image(test_data_dir, test_surprise_imgs[index])\n",
    "    \n",
    "    # 예측\n",
    "    prediction_1 = model_predict(angry_image, model)\n",
    "    prediction_2 = model_predict(disgust_image, model)\n",
    "    prediction_3 = model_predict(fear_image, model)\n",
    "    prediction_4 = model_predict(happy_image, model)\n",
    "    prediction_5 = model_predict(neutral_image, model)\n",
    "    prediction_6 = model_predict(sad_image, model)\n",
    "    prediction_7 = model_predict(surprise_image, model)\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(21, 15))\n",
    "    plt.subplot(171)\n",
    "    plt.title(f'Pred: {feelings_list[prediction_1]} | GT: Angry')\n",
    "    plt.imshow(angry_image)\n",
    "    plt.subplot(172)\n",
    "    plt.title(f'Pred: {feelings_list[prediction_2]} | GT: Disgust')\n",
    "    plt.imshow(disgust_image)\n",
    "    plt.subplot(173)\n",
    "    plt.title(f'Pred: {feelings_list[prediction_3]} | GT: Fear')\n",
    "    plt.imshow(fear_image)\n",
    "    plt.subplot(174)\n",
    "    plt.title(f'Pred: {feelings_list[prediction_4]} | GT: Happy')\n",
    "    plt.imshow(happy_image)\n",
    "    plt.subplot(175)\n",
    "    plt.title(f'Pred: {feelings_list[prediction_5]} | GT: Neutral')\n",
    "    plt.imshow(neutral_image)\n",
    "    plt.subplot(176)\n",
    "    plt.title(f'Pred: {feelings_list[prediction_6]} | GT: Sad')\n",
    "    plt.imshow(sad_image)\n",
    "    plt.subplot(177)\n",
    "    plt.title(f'Pred: {feelings_list[prediction_7]} | GT: Surprise')\n",
    "    plt.imshow(surprise_image)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5449c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
