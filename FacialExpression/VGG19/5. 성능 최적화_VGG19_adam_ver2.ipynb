{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90aa61b5",
   "metadata": {},
   "source": [
    "# **1. 데이터 차원**\n",
    "- 데이터 자체는 충분함\n",
    "- 불균형 데이터 -> Data Augmentation\n",
    "- 데이터 범위(scale) 조정: 정규화/규제화/표준화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c9b54",
   "metadata": {},
   "source": [
    "# **2. 알고리즘 차원**\n",
    "- batch size와 learning rate 사이의 관계가 부적절해 보임(학습이 정체된 것으로 판단된다.)\n",
    "\n",
    "- 학습률(learning rate)\n",
    "    - lr scheduling\n",
    "- 활성화 함수/손실함수\n",
    "    - 활성화 함수: softmax\n",
    "    - 손실 함수: CrossEntropyLoss\n",
    "- 배치/ Epoch\n",
    "    - 충분한 학습이 이루어질 수 있도록 Epoch 수 증가\n",
    "    - batch size: 128\n",
    "    - Epoch: 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89069eb",
   "metadata": {},
   "source": [
    "# **3. 하드웨어 차원**\n",
    "- 역전파처럼 복잡한 미적분 연산의 경우 병렬 연산을 해야 속도/ 정확도 증가\n",
    "- 장치를 cpu에서 gpu로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511168c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 필요한 라이브러리 준비\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import argparse\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from ipywidgets import interact\n",
    "from module.pytorchtools import EarlyStopping # 사용자 정의 모듈\n",
    "                                              # 외부 py파일을 모듈로 import하여 EarlyStopping 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ffb691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu116\n"
     ]
    }
   ],
   "source": [
    "### gpu 장치 확인\n",
    "print(torch.__version__) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc18c93",
   "metadata": {},
   "source": [
    "### **DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5d8b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = './DataSet/archive/train/'\n",
    "train_data_df = pd.read_csv(os.path.join(train_data_dir,'train.csv'))\n",
    "\n",
    "# 분류에 사용할 class 정의(7개의 감정들)\n",
    "feelings_list = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "965a7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 48\n",
    "\n",
    "### 이미지 파일 경로를 리스트 형태로 저장하기 위한 함수\n",
    "def list_image_file(data_dir,sub_dir):\n",
    "    image_files = []\n",
    "    \n",
    "    images_dir = os.path.join(data_dir,sub_dir)\n",
    "    for file_path in os.listdir(images_dir):\n",
    "        image_files.append(os.path.join(sub_dir,file_path))\n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc9ba21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습 데이터셋 클래스\n",
    "class Feeling_dataset(Dataset):\n",
    "    ### 생성자\n",
    "    def __init__(self,data_dir,transform = None):\n",
    "        self.data_dir = data_dir # 데이터가 저장된 상위 directory\n",
    "        angry_imgs = list_image_file(data_dir,'angry')\n",
    "        disgust_imgs = list_image_file(data_dir,'disgust')\n",
    "        fear_imgs = list_image_file(data_dir,'fear')\n",
    "        happy_imgs = list_image_file(data_dir,'happy')\n",
    "        neutral_imgs = list_image_file(data_dir,'neutral')\n",
    "        sad_imgs = list_image_file(data_dir,'sad')\n",
    "        surprise_imgs = list_image_file(data_dir,'surprise')\n",
    "        \n",
    "        # 모든 사진들의 경로를 하나의 리스트에 저장\n",
    "        self.files_path = angry_imgs + disgust_imgs + fear_imgs + happy_imgs + neutral_imgs + sad_imgs + surprise_imgs\n",
    "        self.transform = transform\n",
    "    \n",
    "    ### 데이터 개수 확인\n",
    "    def __len__(self):\n",
    "        return len(self.files_path) # 전체 데이터 개수\n",
    "    \n",
    "    ### getitem\n",
    "    def __getitem__(self,index):\n",
    "        # image(feature data)\n",
    "        image_file = os.path.join(self.data_dir,self.files_path[index])\n",
    "        image = cv2.imread(image_file)\n",
    "        image = cv2.resize(image,dsize = (IMAGE_SIZE,IMAGE_SIZE),interpolation = cv2.INTER_LINEAR)\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # feeling(target data)\n",
    "        target = feelings_list.index(self.files_path[index].split(os.sep)[0])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image) # feature data에 대해서만 데이터 변형 수행\n",
    "            target = torch.Tensor([target]).long()\n",
    "            \n",
    "        return {'image':image,'target':target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f3fde09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformation\n",
    "\n",
    "# 학습 feature data 변환\n",
    "train_transformer = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(), # 수평으로 뒤집기\n",
    "    transforms.RandomVerticalFlip(), # 수직으로 뒤집기\n",
    "    transforms.ToTensor(), # 텐서로 변환\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) # 정규화\n",
    "])\n",
    "\n",
    "# 검증 feature data 변환\n",
    "val_transformer = transforms.Compose([\n",
    "    transforms.ToTensor(), # 텐서로 변환\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) # 정규화\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c087452",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터로더 구현\n",
    "def build_dataloader(train_data_dir,val_data_dir):\n",
    "    dataloaders = {}\n",
    "    train_dset = Feeling_dataset(train_data_dir,train_transformer)\n",
    "    dataloaders['train'] = DataLoader(train_dset,batch_size = 128,shuffle = True,drop_last = True)\n",
    "    \n",
    "    val_dset = Feeling_dataset(val_data_dir,val_transformer)\n",
    "    dataloaders['val'] = DataLoader(val_dset,batch_size = 128,shuffle = False,drop_last = False)\n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30ce035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = './DataSet/archive/train/'\n",
    "val_data_dir = train_data_dir = './DataSet/archive/valid/'\n",
    "dataloaders = build_dataloader(train_data_dir,val_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd26bd",
   "metadata": {},
   "source": [
    "### **Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eca7935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델 생성 함수\n",
    "# 기존의 VGG19 모델 호출 -> head 부분 수정\n",
    "def build_vgg19_based_model():\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # gpu 할당\n",
    "    model = models.vgg19(pretrained = True) # 이미 학습된 vgg19 모델 불러오기\n",
    "    # 일반 NN Layer(FC Layer)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(output_size = (1,1)) # 각 구역의 평균값 출력\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Flatten(), # 평탄화\n",
    "        nn.Linear(512,256), # 512 -> 256\n",
    "        nn.ReLU(), # 활성화 함수\n",
    "        nn.Dropout(0.1), # 과적합 방지\n",
    "        nn.Linear(256,7), # 256 -> 7(7개의 감정으로 분류되니)\n",
    "        nn.Softmax() # 활성화 함수(각 클래스에 속할 확률 추정)\n",
    "    )\n",
    "    \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae75459",
   "metadata": {},
   "source": [
    "### **Estimation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8b8eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델 검증을 위한 accuracy\n",
    "\n",
    "@torch.no_grad() \n",
    "def get_accuracy(image,target,model):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # gpu 할당\n",
    "    batch_size = image.shape[0] \n",
    "    prediction = model(image).to(device) # 예측 \n",
    "    _,pred_label = torch.max(prediction,dim = 1) # 예측이 어느 클래스에 속하는지 확률이 가장 높은 1개 선택\n",
    "    is_correct = (pred_label == target)\n",
    "    \n",
    "    return is_correct.cpu().numpy().sum() / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f48844",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d26e5f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Epoch을 1번 수행\n",
    "\n",
    "def train_one_epoch(dataloaders, model, optimizer, loss_function, device):\n",
    "    losses = {}  # loss값 저장\n",
    "    accuracies = {} # 정확도 계산\n",
    "    \n",
    "    for tv in [\"train\", \"val\"]: \n",
    "        ### loss, accuracy 갱신\n",
    "        running_loss = 0.0 \n",
    "        running_correct = 0.0\n",
    "        \n",
    "        if tv == \"train\":\n",
    "            model.train() # 학습\n",
    "        else:\n",
    "            model.eval() # 평가\n",
    "            \n",
    "            \n",
    "        for index, batch in enumerate(dataloaders[tv]): \n",
    "            image = batch['image'].to(device) # feature data(이미지)\n",
    "            label = batch['target'].squeeze(dim = 1).to(device) # label data(감정), 1차원으로 차원 축소\n",
    "            \n",
    "            ### 역전파 적용\n",
    "            with torch.set_grad_enabled(tv == 'train'):\n",
    "                prediction = model(image) # label 예측\n",
    "                loss = loss_func(prediction, label) # loss 값 계산\n",
    "                \n",
    "                optimizer.zero_grad() # gradient를 0으로 초기화\n",
    "                \n",
    "                if tv == 'train':\n",
    "                    loss.backward() # 역전파 적용\n",
    "                    optimizer.step() # 가중치 업데이트\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "            running_correct += get_accuracy(image, label, model)\n",
    "            \n",
    "            ### 학습 과정 monitoring\n",
    "            if tv == 'train':\n",
    "                if index % 30 == 0:\n",
    "                    print(f\"{index}/{len(dataloaders['train'])} - Running loss: {loss.item()}\")\n",
    "                    \n",
    "        ### loss, accuracy 저장            \n",
    "        losses[tv] = running_loss / len(dataloaders[tv])\n",
    "        accuracies[tv] = running_correct / len(dataloaders[tv])\n",
    "        \n",
    "    return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb3fd788",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습이 잘 된 모델 저장\n",
    "def save_best_model(model_state,model_name,save_dir = './best_model'):\n",
    "    os.makedirs(save_dir,exist_ok = True) # 경로 존재 시 덮어쓰기, 없는 경우 새로 생성\n",
    "    torch.save(model_state,os.path.join(save_dir,model_name)) # 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac16bd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\users\\bin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # 장치 얻어오기\n",
    "\n",
    "### 경로 설정\n",
    "train_data_dir = './DataSet/archive/train/'\n",
    "val_data_dir = './DataSet/archive/valid/'\n",
    "\n",
    "### 필요한 요소들 준비\n",
    "dataloaders = build_dataloader(train_data_dir,val_data_dir)\n",
    "model = build_vgg19_based_model().to(device)\n",
    "loss_func = nn.CrossEntropyLoss(reduction = 'mean').to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', patience = 5, factor = 0.1,\n",
    "                                                       min_lr = 1e-8,verbose = True)  # lr scheduling\n",
    "early_stopping = EarlyStopping(patience = 10, verbose = False) # 조기 종료(사용자 정의 모듈)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b285105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/227 - Running loss: 1.9463222026824951\n",
      "30/227 - Running loss: 1.891983985900879\n",
      "60/227 - Running loss: 1.891983985900879\n",
      "90/227 - Running loss: 1.954483985900879\n",
      "120/227 - Running loss: 1.923233985900879\n",
      "150/227 - Running loss: 1.899796485900879\n",
      "180/227 - Running loss: 1.915421485900879\n",
      "210/227 - Running loss: 1.946671485900879\n",
      "1/200-Train Loss:1.9194603937838046, Val Loss:1.923234121552829\n",
      "1/200-Train Acc:0.24593887665198239, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.954483985900879\n",
      "30/227 - Running loss: 2.009171485900879\n",
      "60/227 - Running loss: 1.954483985900879\n",
      "90/227 - Running loss: 1.899796485900879\n",
      "120/227 - Running loss: 1.977921485900879\n",
      "150/227 - Running loss: 1.946671485900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "2/200-Train Loss:1.9187942801068008, Val Loss:1.923234121552829\n",
      "2/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.907608985900879\n",
      "30/227 - Running loss: 1.915421485900879\n",
      "60/227 - Running loss: 1.852921485900879\n",
      "90/227 - Running loss: 1.946671485900879\n",
      "120/227 - Running loss: 1.845108985900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.915421485900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "3/200-Train Loss:1.9184157013367977, Val Loss:1.923234121552829\n",
      "3/200-Train Acc:0.24700578193832598, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.868546485900879\n",
      "30/227 - Running loss: 1.868546485900879\n",
      "60/227 - Running loss: 1.970108985900879\n",
      "90/227 - Running loss: 1.868546485900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.954483985900879\n",
      "180/227 - Running loss: 1.899796485900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "4/200-Train Loss:1.9188975248042708, Val Loss:1.923234121552829\n",
      "4/200-Train Acc:0.2465239537444934, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.923233985900879\n",
      "60/227 - Running loss: 1.876358985900879\n",
      "90/227 - Running loss: 1.876358985900879\n",
      "120/227 - Running loss: 1.868546485900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 1.970108985900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "5/200-Train Loss:1.9184845297347082, Val Loss:1.923234121552829\n",
      "5/200-Train Acc:0.24693694933920704, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.938858985900879\n",
      "30/227 - Running loss: 1.954483985900879\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.884171485900879\n",
      "120/227 - Running loss: 1.868546485900879\n",
      "150/227 - Running loss: 1.954483985900879\n",
      "180/227 - Running loss: 1.938858985900879\n",
      "210/227 - Running loss: 1.899796485900879\n",
      "6/200-Train Loss:1.918691027532065, Val Loss:1.923234121552829\n",
      "6/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.907608985900879\n",
      "30/227 - Running loss: 1.938858985900879\n",
      "60/227 - Running loss: 1.891983985900879\n",
      "90/227 - Running loss: 1.876358985900879\n",
      "120/227 - Running loss: 1.923233985900879\n",
      "150/227 - Running loss: 1.962296485900879\n",
      "180/227 - Running loss: 1.931046485900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "7/200-Train Loss:1.9185877791585375, Val Loss:1.923234121552829\n",
      "7/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.8529216051101685\n",
      "30/227 - Running loss: 1.899796485900879\n",
      "60/227 - Running loss: 1.868546485900879\n",
      "90/227 - Running loss: 1.899796485900879\n",
      "120/227 - Running loss: 1.9622963666915894\n",
      "150/227 - Running loss: 1.860733985900879\n",
      "180/227 - Running loss: 1.899796485900879\n",
      "210/227 - Running loss: 1.852921485900879\n",
      "8/200-Train Loss:1.9184845323604633, Val Loss:1.923234121552829\n",
      "8/200-Train Acc:0.24693694933920704, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.954483985900879\n",
      "30/227 - Running loss: 1.884171485900879\n",
      "60/227 - Running loss: 1.962296485900879\n",
      "90/227 - Running loss: 1.852921485900879\n",
      "120/227 - Running loss: 1.954483985900879\n",
      "150/227 - Running loss: 1.876358985900879\n",
      "180/227 - Running loss: 1.938858985900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "9/200-Train Loss:1.9187254422561713, Val Loss:1.923234121552829\n",
      "9/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.8685466051101685\n",
      "30/227 - Running loss: 1.907608985900879\n",
      "60/227 - Running loss: 1.915421485900879\n",
      "90/227 - Running loss: 1.970108985900879\n",
      "120/227 - Running loss: 1.9701088666915894\n",
      "150/227 - Running loss: 1.915421485900879\n",
      "180/227 - Running loss: 1.876358985900879\n",
      "210/227 - Running loss: 1.9779213666915894\n",
      "10/200-Train Loss:1.9184845297347082, Val Loss:1.923234121552829\n",
      "10/200-Train Acc:0.24693694933920704, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.860733985900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.915421485900879\n",
      "90/227 - Running loss: 1.891983985900879\n",
      "120/227 - Running loss: 1.845108985900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 1.884171485900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "11/200-Train Loss:1.9185877796836888, Val Loss:1.923234121552829\n",
      "11/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.8372966051101685\n",
      "30/227 - Running loss: 1.899796485900879\n",
      "60/227 - Running loss: 2.001358985900879\n",
      "90/227 - Running loss: 1.946671485900879\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 1.946671485900879\n",
      "180/227 - Running loss: 1.907608985900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "12/200-Train Loss:1.918691028582367, Val Loss:1.923234121552829\n",
      "12/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.938858985900879\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.931046485900879\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.946671485900879\n",
      "180/227 - Running loss: 1.868546485900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n",
      "13/200-Train Loss:1.9184157002864954, Val Loss:1.923234121552829\n",
      "13/200-Train Acc:0.24700578193832598, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.899796485900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.845108985900879\n",
      "90/227 - Running loss: 1.931046485900879\n",
      "120/227 - Running loss: 1.954483985900879\n",
      "150/227 - Running loss: 1.891983985900879\n",
      "180/227 - Running loss: 1.954483985900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "14/200-Train Loss:1.9185533633841292, Val Loss:1.923234121552829\n",
      "14/200-Train Acc:0.2468681167400881, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.868546485900879\n",
      "60/227 - Running loss: 1.907608985900879\n",
      "90/227 - Running loss: 1.876358985900879\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.946671485900879\n",
      "210/227 - Running loss: 1.852921485900879\n",
      "15/200-Train Loss:1.9187942780061966, Val Loss:1.923234121552829\n",
      "15/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 2.009171485900879\n",
      "60/227 - Running loss: 1.907608985900879\n",
      "90/227 - Running loss: 1.915421485900879\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.899796485900879\n",
      "16/200-Train Loss:1.9186566122828077, Val Loss:1.923234121552829\n",
      "16/200-Train Acc:0.2467648678414097, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.954483985900879\n",
      "30/227 - Running loss: 1.899796485900879\n",
      "60/227 - Running loss: 1.8138591051101685\n",
      "90/227 - Running loss: 1.954483985900879\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.915421485900879\n",
      "180/227 - Running loss: 1.954483985900879\n",
      "210/227 - Running loss: 1.946671485900879\n",
      "17/200-Train Loss:1.9186910312081225, Val Loss:1.923234121552829\n",
      "17/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.9935463666915894\n",
      "60/227 - Running loss: 1.946671485900879\n",
      "90/227 - Running loss: 1.946671485900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 1.946671485900879\n",
      "210/227 - Running loss: 1.884171485900879\n",
      "18/200-Train Loss:1.918759859606033, Val Loss:1.923234121552829\n",
      "18/200-Train Acc:0.24666161894273128, Val Acc:0.2421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/227 - Running loss: 1.946671485900879\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.852921485900879\n",
      "90/227 - Running loss: 1.915421485900879\n",
      "120/227 - Running loss: 1.852921485900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.899796485900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n",
      "19/200-Train Loss:1.9187598638072414, Val Loss:1.923234121552829\n",
      "19/200-Train Acc:0.24666161894273128, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.915421485900879\n",
      "60/227 - Running loss: 1.915421485900879\n",
      "90/227 - Running loss: 1.938858985900879\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 2.001358985900879\n",
      "210/227 - Running loss: 1.829483985900879\n",
      "20/200-Train Loss:1.918622193882644, Val Loss:1.923234121552829\n",
      "20/200-Train Acc:0.24679928414096916, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.985733985900879\n",
      "30/227 - Running loss: 1.9544838666915894\n",
      "60/227 - Running loss: 1.993546485900879\n",
      "90/227 - Running loss: 1.899796485900879\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 1.860733985900879\n",
      "180/227 - Running loss: 1.970108985900879\n",
      "210/227 - Running loss: 1.970108985900879\n",
      "21/200-Train Loss:1.918828695881209, Val Loss:1.923234121552829\n",
      "21/200-Train Acc:0.24659278634361234, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.938858985900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.923233985900879\n",
      "150/227 - Running loss: 1.946671485900879\n",
      "180/227 - Running loss: 1.954483985900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "22/200-Train Loss:1.9185877812591419, Val Loss:1.923234121552829\n",
      "22/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.899796485900879\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.946671485900879\n",
      "90/227 - Running loss: 1.907608985900879\n",
      "120/227 - Running loss: 1.884171485900879\n",
      "150/227 - Running loss: 1.868546485900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.962296485900879\n",
      "23/200-Train Loss:1.9187942822074049, Val Loss:1.923234121552829\n",
      "23/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.923233985900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.938858985900879\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 1.891983985900879\n",
      "180/227 - Running loss: 1.931046485900879\n",
      "210/227 - Running loss: 2.001358985900879\n",
      "24/200-Train Loss:1.918828694830907, Val Loss:1.923234121552829\n",
      "24/200-Train Acc:0.24659278634361234, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.923233985900879\n",
      "60/227 - Running loss: 1.852921485900879\n",
      "90/227 - Running loss: 1.946671485900879\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.884171485900879\n",
      "180/227 - Running loss: 1.970108985900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-07.\n",
      "25/200-Train Loss:1.9185189460342675, Val Loss:1.923234121552829\n",
      "25/200-Train Acc:0.24690253303964757, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.899796485900879\n",
      "60/227 - Running loss: 1.907608985900879\n",
      "90/227 - Running loss: 1.899796485900879\n",
      "120/227 - Running loss: 1.946671485900879\n",
      "150/227 - Running loss: 1.891983985900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "26/200-Train Loss:1.9186221970335502, Val Loss:1.923234121552829\n",
      "26/200-Train Acc:0.24679928414096916, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.891983985900879\n",
      "60/227 - Running loss: 1.915421485900879\n",
      "90/227 - Running loss: 1.891983985900879\n",
      "120/227 - Running loss: 1.868546485900879\n",
      "150/227 - Running loss: 1.884171485900879\n",
      "180/227 - Running loss: 1.938858985900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "27/200-Train Loss:1.9188631106053156, Val Loss:1.923234121552829\n",
      "27/200-Train Acc:0.24655837004405287, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.9622963666915894\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.954483985900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.938858985900879\n",
      "180/227 - Running loss: 1.915421485900879\n",
      "210/227 - Running loss: 1.845108985900879\n",
      "28/200-Train Loss:1.9186566133331098, Val Loss:1.923234121552829\n",
      "28/200-Train Acc:0.2467648678414097, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.970108985900879\n",
      "30/227 - Running loss: 2.009171485900879\n",
      "60/227 - Running loss: 1.946671485900879\n",
      "90/227 - Running loss: 1.931046485900879\n",
      "120/227 - Running loss: 1.923233985900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.931046485900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "29/200-Train Loss:1.9186910306829712, Val Loss:1.923234121552829\n",
      "29/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.876358985900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.852921485900879\n",
      "90/227 - Running loss: 1.884171485900879\n",
      "120/227 - Running loss: 1.8138591051101685\n",
      "150/227 - Running loss: 2.001358985900879\n",
      "180/227 - Running loss: 1.845108985900879\n",
      "210/227 - Running loss: 1.931046485900879\n",
      "30/200-Train Loss:1.9185877812591419, Val Loss:1.923234121552829\n",
      "30/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.907608985900879\n",
      "30/227 - Running loss: 1.970108985900879\n",
      "60/227 - Running loss: 1.868546485900879\n",
      "90/227 - Running loss: 1.845108985900879\n",
      "120/227 - Running loss: 1.899796485900879\n",
      "150/227 - Running loss: 1.891983985900879\n",
      "180/227 - Running loss: 1.852921485900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-08.\n",
      "31/200-Train Loss:1.9183468692628298, Val Loss:1.923234121552829\n",
      "31/200-Train Acc:0.24707461453744495, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.938858985900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.899796485900879\n",
      "90/227 - Running loss: 1.899796485900879\n",
      "120/227 - Running loss: 1.868546485900879\n",
      "150/227 - Running loss: 1.899796485900879\n",
      "180/227 - Running loss: 1.954483985900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "32/200-Train Loss:1.918828693780605, Val Loss:1.923234121552829\n",
      "32/200-Train Acc:0.24659278634361234, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.884171485900879\n",
      "30/227 - Running loss: 1.923233985900879\n",
      "60/227 - Running loss: 1.891983985900879\n",
      "90/227 - Running loss: 1.852921485900879\n",
      "120/227 - Running loss: 1.9701088666915894\n",
      "150/227 - Running loss: 1.946671485900879\n",
      "180/227 - Running loss: 1.915421485900879\n",
      "210/227 - Running loss: 1.938858985900879\n",
      "33/200-Train Loss:1.9186221991341545, Val Loss:1.923234121552829\n",
      "33/200-Train Acc:0.24679928414096916, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.884171485900879\n",
      "30/227 - Running loss: 1.938858985900879\n",
      "60/227 - Running loss: 1.837296485900879\n",
      "90/227 - Running loss: 1.915421485900879\n",
      "120/227 - Running loss: 1.9701088666915894\n",
      "150/227 - Running loss: 1.954483985900879\n",
      "180/227 - Running loss: 1.806046485900879\n",
      "210/227 - Running loss: 1.876358985900879\n",
      "34/200-Train Loss:1.9187598590808819, Val Loss:1.923234121552829\n",
      "34/200-Train Acc:0.24666161894273128, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.891983985900879\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.946671485900879\n",
      "120/227 - Running loss: 1.899796485900879\n",
      "150/227 - Running loss: 1.946671485900879\n",
      "180/227 - Running loss: 1.829483985900879\n",
      "210/227 - Running loss: 1.899796485900879\n",
      "35/200-Train Loss:1.918828695356058, Val Loss:1.923234121552829\n",
      "35/200-Train Acc:0.24659278634361234, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.954483985900879\n",
      "30/227 - Running loss: 1.884171485900879\n",
      "60/227 - Running loss: 1.962296485900879\n",
      "90/227 - Running loss: 1.891983985900879\n",
      "120/227 - Running loss: 1.884171485900879\n",
      "150/227 - Running loss: 1.860733985900879\n",
      "180/227 - Running loss: 1.946671485900879\n",
      "210/227 - Running loss: 1.946671485900879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/200-Train Loss:1.9187942790564987, Val Loss:1.923234121552829\n",
      "36/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.907608985900879\n",
      "30/227 - Running loss: 1.899796485900879\n",
      "60/227 - Running loss: 1.915421485900879\n",
      "90/227 - Running loss: 1.954483985900879\n",
      "120/227 - Running loss: 1.993546485900879\n",
      "150/227 - Running loss: 1.970108985900879\n",
      "180/227 - Running loss: 1.915421485900879\n",
      "210/227 - Running loss: 1.954483985900879\n",
      "37/200-Train Loss:1.9187254506585882, Val Loss:1.923234121552829\n",
      "37/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.884171485900879\n",
      "60/227 - Running loss: 1.860733985900879\n",
      "90/227 - Running loss: 1.9544838666915894\n",
      "120/227 - Running loss: 1.852921485900879\n",
      "150/227 - Running loss: 1.915421485900879\n",
      "180/227 - Running loss: 1.907608985900879\n",
      "210/227 - Running loss: 1.868546485900879\n",
      "38/200-Train Loss:1.9187254443567754, Val Loss:1.923234121552829\n",
      "38/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.884171485900879\n",
      "30/227 - Running loss: 1.876358985900879\n",
      "60/227 - Running loss: 1.891983985900879\n",
      "90/227 - Running loss: 1.915421485900879\n",
      "120/227 - Running loss: 1.9779213666915894\n",
      "150/227 - Running loss: 1.852921485900879\n",
      "180/227 - Running loss: 1.907608985900879\n",
      "210/227 - Running loss: 1.938858985900879\n",
      "39/200-Train Loss:1.918691027006914, Val Loss:1.923234121552829\n",
      "39/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.946671485900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.868546485900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.946671485900879\n",
      "210/227 - Running loss: 1.931046485900879\n",
      "40/200-Train Loss:1.9185533644344313, Val Loss:1.923234121552829\n",
      "40/200-Train Acc:0.2468681167400881, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.923233985900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.8294841051101685\n",
      "120/227 - Running loss: 1.954483985900879\n",
      "150/227 - Running loss: 1.860733985900879\n",
      "180/227 - Running loss: 1.946671485900879\n",
      "210/227 - Running loss: 1.876358985900879\n",
      "41/200-Train Loss:1.9186910322584245, Val Loss:1.923234121552829\n",
      "41/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.907608985900879\n",
      "30/227 - Running loss: 1.923233985900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.970108985900879\n",
      "120/227 - Running loss: 1.884171485900879\n",
      "150/227 - Running loss: 1.899796485900879\n",
      "180/227 - Running loss: 1.899796485900879\n",
      "210/227 - Running loss: 1.860733985900879\n",
      "42/200-Train Loss:1.9187598643323924, Val Loss:1.923234121552829\n",
      "42/200-Train Acc:0.24666161894273128, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.891983985900879\n",
      "30/227 - Running loss: 1.962296485900879\n",
      "60/227 - Running loss: 1.9622963666915894\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.884171485900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "43/200-Train Loss:1.9187254448819266, Val Loss:1.923234121552829\n",
      "43/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.938858985900879\n",
      "60/227 - Running loss: 1.962296485900879\n",
      "90/227 - Running loss: 1.907608985900879\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 1.899796485900879\n",
      "180/227 - Running loss: 1.9622963666915894\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "44/200-Train Loss:1.9189319432044345, Val Loss:1.923234121552829\n",
      "44/200-Train Acc:0.24648953744493393, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.9779213666915894\n",
      "60/227 - Running loss: 1.899796485900879\n",
      "90/227 - Running loss: 1.938858985900879\n",
      "120/227 - Running loss: 1.962296485900879\n",
      "150/227 - Running loss: 1.884171485900879\n",
      "180/227 - Running loss: 1.845108985900879\n",
      "210/227 - Running loss: 1.946671485900879\n",
      "45/200-Train Loss:1.9185189476097209, Val Loss:1.923234121552829\n",
      "45/200-Train Acc:0.24690253303964757, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.907608985900879\n",
      "30/227 - Running loss: 1.923233985900879\n",
      "60/227 - Running loss: 1.985733985900879\n",
      "90/227 - Running loss: 1.907608985900879\n",
      "120/227 - Running loss: 1.923233985900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.931046485900879\n",
      "210/227 - Running loss: 1.884171485900879\n",
      "46/200-Train Loss:1.9188631090298622, Val Loss:1.923234121552829\n",
      "46/200-Train Acc:0.24655837004405287, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.8294841051101685\n",
      "90/227 - Running loss: 1.946671485900879\n",
      "120/227 - Running loss: 1.852921485900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "47/200-Train Loss:1.9186221959832481, Val Loss:1.923234121552829\n",
      "47/200-Train Acc:0.24679928414096916, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.970108985900879\n",
      "30/227 - Running loss: 1.852921485900879\n",
      "60/227 - Running loss: 1.970108985900879\n",
      "90/227 - Running loss: 1.915421485900879\n",
      "120/227 - Running loss: 1.9388588666915894\n",
      "150/227 - Running loss: 1.915421485900879\n",
      "180/227 - Running loss: 1.852921485900879\n",
      "210/227 - Running loss: 1.931046485900879\n",
      "48/200-Train Loss:1.918691027532065, Val Loss:1.923234121552829\n",
      "48/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.899796485900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.954483985900879\n",
      "90/227 - Running loss: 1.9544838666915894\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.884171485900879\n",
      "180/227 - Running loss: 1.938858985900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "49/200-Train Loss:1.9185877796836888, Val Loss:1.923234121552829\n",
      "49/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.868546485900879\n",
      "60/227 - Running loss: 1.884171485900879\n",
      "90/227 - Running loss: 1.931046485900879\n",
      "120/227 - Running loss: 1.931046485900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 1.884171485900879\n",
      "210/227 - Running loss: 1.868546485900879\n",
      "50/200-Train Loss:1.9186566107073544, Val Loss:1.923234121552829\n",
      "50/200-Train Acc:0.2467648678414097, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.860733985900879\n",
      "30/227 - Running loss: 1.899796485900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.954483985900879\n",
      "150/227 - Running loss: 1.899796485900879\n",
      "180/227 - Running loss: 1.938858985900879\n",
      "210/227 - Running loss: 1.860733985900879\n",
      "51/200-Train Loss:1.9186910249063096, Val Loss:1.923234121552829\n",
      "51/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.884171485900879\n",
      "30/227 - Running loss: 1.907608985900879\n",
      "60/227 - Running loss: 1.868546485900879\n",
      "90/227 - Running loss: 1.899796485900879\n",
      "120/227 - Running loss: 1.868546485900879\n",
      "150/227 - Running loss: 1.899796485900879\n",
      "180/227 - Running loss: 1.845108985900879\n",
      "210/227 - Running loss: 1.938858985900879\n",
      "52/200-Train Loss:1.9187254438316244, Val Loss:1.923234121552829\n",
      "52/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.907608985900879\n",
      "30/227 - Running loss: 1.915421485900879\n",
      "60/227 - Running loss: 1.845108985900879\n",
      "90/227 - Running loss: 1.907608985900879\n",
      "120/227 - Running loss: 1.845108985900879\n",
      "150/227 - Running loss: 1.962296485900879\n",
      "180/227 - Running loss: 1.806046485900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "53/200-Train Loss:1.9186910306829712, Val Loss:1.923234121552829\n",
      "53/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.938858985900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.954483985900879\n",
      "90/227 - Running loss: 1.845108985900879\n",
      "120/227 - Running loss: 1.954483985900879\n",
      "150/227 - Running loss: 1.915421485900879\n",
      "180/227 - Running loss: 1.985733985900879\n",
      "210/227 - Running loss: 1.946671485900879\n",
      "54/200-Train Loss:1.9185877812591419, Val Loss:1.923234121552829\n",
      "54/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/227 - Running loss: 1.946671485900879\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.915421485900879\n",
      "90/227 - Running loss: 1.931046485900879\n",
      "120/227 - Running loss: 1.923233985900879\n",
      "150/227 - Running loss: 1.891983985900879\n",
      "180/227 - Running loss: 1.970108985900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "55/200-Train Loss:1.9185533633841292, Val Loss:1.923234121552829\n",
      "55/200-Train Acc:0.2468681167400881, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.876358985900879\n",
      "30/227 - Running loss: 1.829483985900879\n",
      "60/227 - Running loss: 1.993546485900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.868546485900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.938858985900879\n",
      "56/200-Train Loss:1.9188286916800008, Val Loss:1.923234121552829\n",
      "56/200-Train Acc:0.24659278634361234, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.907608985900879\n",
      "60/227 - Running loss: 1.915421485900879\n",
      "90/227 - Running loss: 2.001358985900879\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 1.884171485900879\n",
      "180/227 - Running loss: 1.931046485900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "57/200-Train Loss:1.9186910296326691, Val Loss:1.923234121552829\n",
      "57/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.907608985900879\n",
      "30/227 - Running loss: 1.9779213666915894\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.852921485900879\n",
      "120/227 - Running loss: 1.899796485900879\n",
      "150/227 - Running loss: 1.9935463666915894\n",
      "180/227 - Running loss: 1.821671485900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "58/200-Train Loss:1.9185533623338271, Val Loss:1.923234121552829\n",
      "58/200-Train Acc:0.2468681167400881, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.977921485900879\n",
      "60/227 - Running loss: 1.907608985900879\n",
      "90/227 - Running loss: 1.891983985900879\n",
      "120/227 - Running loss: 1.923233985900879\n",
      "150/227 - Running loss: 1.899796485900879\n",
      "180/227 - Running loss: 1.938858985900879\n",
      "210/227 - Running loss: 1.946671485900879\n",
      "59/200-Train Loss:1.9186910296326691, Val Loss:1.923234121552829\n",
      "59/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.852921485900879\n",
      "30/227 - Running loss: 1.860733985900879\n",
      "60/227 - Running loss: 1.876358985900879\n",
      "90/227 - Running loss: 1.899796485900879\n",
      "120/227 - Running loss: 1.837296485900879\n",
      "150/227 - Running loss: 1.915421485900879\n",
      "180/227 - Running loss: 1.860733985900879\n",
      "210/227 - Running loss: 1.852921485900879\n",
      "60/200-Train Loss:1.9185533665350356, Val Loss:1.923234121552829\n",
      "60/200-Train Acc:0.2468681167400881, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.884171485900879\n",
      "30/227 - Running loss: 1.915421485900879\n",
      "60/227 - Running loss: 1.899796485900879\n",
      "90/227 - Running loss: 1.868546485900879\n",
      "120/227 - Running loss: 1.970108985900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.9935463666915894\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "61/200-Train Loss:1.9186910306829712, Val Loss:1.923234121552829\n",
      "61/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.907608985900879\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.899796485900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.946671485900879\n",
      "180/227 - Running loss: 1.9388588666915894\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "62/200-Train Loss:1.9187942759055923, Val Loss:1.923234121552829\n",
      "62/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.868546485900879\n",
      "60/227 - Running loss: 1.891983985900879\n",
      "90/227 - Running loss: 1.9154213666915894\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 2.001358985900879\n",
      "180/227 - Running loss: 1.845108985900879\n",
      "210/227 - Running loss: 1.954483985900879\n",
      "63/200-Train Loss:1.9187942764307433, Val Loss:1.923234121552829\n",
      "63/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.954483985900879\n",
      "30/227 - Running loss: 1.907608985900879\n",
      "60/227 - Running loss: 1.899796485900879\n",
      "90/227 - Running loss: 1.946671485900879\n",
      "120/227 - Running loss: 1.931046485900879\n",
      "150/227 - Running loss: 1.938858985900879\n",
      "180/227 - Running loss: 1.884171485900879\n",
      "210/227 - Running loss: 1.876358985900879\n",
      "64/200-Train Loss:1.9187942811571028, Val Loss:1.923234121552829\n",
      "64/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.970108985900879\n",
      "30/227 - Running loss: 1.891983985900879\n",
      "60/227 - Running loss: 1.946671485900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.899796485900879\n",
      "150/227 - Running loss: 1.9779213666915894\n",
      "180/227 - Running loss: 1.931046485900879\n",
      "210/227 - Running loss: 1.970108985900879\n",
      "65/200-Train Loss:1.918828694305756, Val Loss:1.923234121552829\n",
      "65/200-Train Acc:0.24659278634361234, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.884171485900879\n",
      "60/227 - Running loss: 1.891983985900879\n",
      "90/227 - Running loss: 1.907608985900879\n",
      "120/227 - Running loss: 1.962296485900879\n",
      "150/227 - Running loss: 1.938858985900879\n",
      "180/227 - Running loss: 1.876358985900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "66/200-Train Loss:1.9187254464573797, Val Loss:1.923234121552829\n",
      "66/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.923233985900879\n",
      "60/227 - Running loss: 1.962296485900879\n",
      "90/227 - Running loss: 1.868546485900879\n",
      "120/227 - Running loss: 1.829483985900879\n",
      "150/227 - Running loss: 1.946671485900879\n",
      "180/227 - Running loss: 1.954483985900879\n",
      "210/227 - Running loss: 1.931046485900879\n",
      "67/200-Train Loss:1.9186566138582608, Val Loss:1.923234121552829\n",
      "67/200-Train Acc:0.2467648678414097, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.891983985900879\n",
      "30/227 - Running loss: 1.954483985900879\n",
      "60/227 - Running loss: 1.891983985900879\n",
      "90/227 - Running loss: 1.938858985900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.868546485900879\n",
      "180/227 - Running loss: 1.962296485900879\n",
      "210/227 - Running loss: 1.946671485900879\n",
      "68/200-Train Loss:1.918897527955177, Val Loss:1.923234121552829\n",
      "68/200-Train Acc:0.2465239537444934, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.899796485900879\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 2.001358985900879\n",
      "210/227 - Running loss: 1.946671485900879\n",
      "69/200-Train Loss:1.918828694305756, Val Loss:1.923234121552829\n",
      "69/200-Train Acc:0.24659278634361234, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.946671485900879\n",
      "30/227 - Running loss: 1.837296485900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.876358985900879\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.884171485900879\n",
      "180/227 - Running loss: 1.993546485900879\n",
      "210/227 - Running loss: 1.868546485900879\n",
      "70/200-Train Loss:1.9187254427813223, Val Loss:1.923234121552829\n",
      "70/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.876358985900879\n",
      "90/227 - Running loss: 2.024796485900879\n",
      "120/227 - Running loss: 1.845108985900879\n",
      "150/227 - Running loss: 1.931046485900879\n",
      "180/227 - Running loss: 1.985733985900879\n",
      "210/227 - Running loss: 1.868546485900879\n",
      "71/200-Train Loss:1.9187942780061966, Val Loss:1.923234121552829\n",
      "71/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.923233985900879\n",
      "60/227 - Running loss: 1.907608985900879\n",
      "90/227 - Running loss: 1.946671485900879\n",
      "120/227 - Running loss: 1.923233985900879\n",
      "150/227 - Running loss: 1.931046485900879\n",
      "180/227 - Running loss: 1.907608985900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "72/200-Train Loss:1.9186566138582608, Val Loss:1.923234121552829\n",
      "72/200-Train Acc:0.2467648678414097, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.876358985900879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/227 - Running loss: 1.946671485900879\n",
      "90/227 - Running loss: 1.9310463666915894\n",
      "120/227 - Running loss: 1.985733985900879\n",
      "150/227 - Running loss: 1.915421485900879\n",
      "180/227 - Running loss: 1.891983985900879\n",
      "210/227 - Running loss: 1.9701088666915894\n",
      "73/200-Train Loss:1.9184845307850102, Val Loss:1.923234121552829\n",
      "73/200-Train Acc:0.24693694933920704, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.946671485900879\n",
      "30/227 - Running loss: 1.962296485900879\n",
      "60/227 - Running loss: 1.915421485900879\n",
      "90/227 - Running loss: 1.931046485900879\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.876358985900879\n",
      "180/227 - Running loss: 1.970108985900879\n",
      "210/227 - Running loss: 1.938858985900879\n",
      "74/200-Train Loss:1.9185533649595823, Val Loss:1.923234121552829\n",
      "74/200-Train Acc:0.2468681167400881, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.884171485900879\n",
      "60/227 - Running loss: 1.868546485900879\n",
      "90/227 - Running loss: 1.915421485900879\n",
      "120/227 - Running loss: 1.946671485900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 1.946671485900879\n",
      "210/227 - Running loss: 1.884171485900879\n",
      "75/200-Train Loss:1.918656617009167, Val Loss:1.923234121552829\n",
      "75/200-Train Acc:0.2467648678414097, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.868546485900879\n",
      "60/227 - Running loss: 1.860733985900879\n",
      "90/227 - Running loss: 1.907608985900879\n",
      "120/227 - Running loss: 1.899796485900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 1.868546485900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "76/200-Train Loss:1.9185533670601866, Val Loss:1.923234121552829\n",
      "76/200-Train Acc:0.2468681167400881, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.876358985900879\n",
      "60/227 - Running loss: 1.938858985900879\n",
      "90/227 - Running loss: 1.9544838666915894\n",
      "120/227 - Running loss: 1.931046485900879\n",
      "150/227 - Running loss: 1.868546485900879\n",
      "180/227 - Running loss: 1.868546485900879\n",
      "210/227 - Running loss: 1.899796485900879\n",
      "77/200-Train Loss:1.9188631116556176, Val Loss:1.923234121552829\n",
      "77/200-Train Acc:0.24655837004405287, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.985733985900879\n",
      "30/227 - Running loss: 1.876358985900879\n",
      "60/227 - Running loss: 1.946671485900879\n",
      "90/227 - Running loss: 1.868546485900879\n",
      "120/227 - Running loss: 1.852921485900879\n",
      "150/227 - Running loss: 1.884171485900879\n",
      "180/227 - Running loss: 1.891983985900879\n",
      "210/227 - Running loss: 1.938858985900879\n",
      "78/200-Train Loss:1.918897527955177, Val Loss:1.923234121552829\n",
      "78/200-Train Acc:0.2465239537444934, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.954483985900879\n",
      "30/227 - Running loss: 1.915421485900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.9154213666915894\n",
      "120/227 - Running loss: 1.899796485900879\n",
      "150/227 - Running loss: 1.931046485900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "79/200-Train Loss:1.9185189470845698, Val Loss:1.923234121552829\n",
      "79/200-Train Acc:0.24690253303964757, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.977921485900879\n",
      "60/227 - Running loss: 1.977921485900879\n",
      "90/227 - Running loss: 1.946671485900879\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.876358985900879\n",
      "180/227 - Running loss: 1.931046485900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "80/200-Train Loss:1.9188286916800008, Val Loss:1.923234121552829\n",
      "80/200-Train Acc:0.24659278634361234, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.891983985900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.891983985900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.891983985900879\n",
      "180/227 - Running loss: 1.876358985900879\n",
      "210/227 - Running loss: 1.9544838666915894\n",
      "81/200-Train Loss:1.9185877791585375, Val Loss:1.923234121552829\n",
      "81/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.891983985900879\n",
      "30/227 - Running loss: 1.9466713666915894\n",
      "60/227 - Running loss: 1.9310463666915894\n",
      "90/227 - Running loss: 1.915421485900879\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 1.860733985900879\n",
      "180/227 - Running loss: 1.931046485900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "82/200-Train Loss:1.9187942795816497, Val Loss:1.923234121552829\n",
      "82/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.860733985900879\n",
      "30/227 - Running loss: 1.923233985900879\n",
      "60/227 - Running loss: 1.962296485900879\n",
      "90/227 - Running loss: 1.977921485900879\n",
      "120/227 - Running loss: 1.868546485900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.907608985900879\n",
      "210/227 - Running loss: 1.899796485900879\n",
      "83/200-Train Loss:1.918897527955177, Val Loss:1.923234121552829\n",
      "83/200-Train Acc:0.2465239537444934, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.899796485900879\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.907608985900879\n",
      "90/227 - Running loss: 2.001358985900879\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.821671485900879\n",
      "180/227 - Running loss: 1.915421485900879\n",
      "210/227 - Running loss: 1.837296485900879\n",
      "84/200-Train Loss:1.9185877791585375, Val Loss:1.923234121552829\n",
      "84/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.860733985900879\n",
      "30/227 - Running loss: 1.915421485900879\n",
      "60/227 - Running loss: 1.962296485900879\n",
      "90/227 - Running loss: 2.001358985900879\n",
      "120/227 - Running loss: 1.876358985900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.860733985900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "85/200-Train Loss:1.9185877812591419, Val Loss:1.923234121552829\n",
      "85/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.970108985900879\n",
      "30/227 - Running loss: 1.954483985900879\n",
      "60/227 - Running loss: 1.946671485900879\n",
      "90/227 - Running loss: 1.962296485900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.931046485900879\n",
      "180/227 - Running loss: 1.860733985900879\n",
      "210/227 - Running loss: 1.876358985900879\n",
      "86/200-Train Loss:1.918622194932946, Val Loss:1.923234121552829\n",
      "86/200-Train Acc:0.24679928414096916, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.899796485900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.899796485900879\n",
      "90/227 - Running loss: 1.931046485900879\n",
      "120/227 - Running loss: 1.923233985900879\n",
      "150/227 - Running loss: 1.891983985900879\n",
      "180/227 - Running loss: 1.891983985900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "87/200-Train Loss:1.9183468666370744, Val Loss:1.923234121552829\n",
      "87/200-Train Acc:0.24707461453744495, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.970108985900879\n",
      "30/227 - Running loss: 1.993546485900879\n",
      "60/227 - Running loss: 1.899796485900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.891983985900879\n",
      "180/227 - Running loss: 1.899796485900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "88/200-Train Loss:1.9187942790564987, Val Loss:1.923234121552829\n",
      "88/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.852921485900879\n",
      "30/227 - Running loss: 1.884171485900879\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.938858985900879\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 1.884171485900879\n",
      "180/227 - Running loss: 1.907608985900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "89/200-Train Loss:1.918622194407795, Val Loss:1.923234121552829\n",
      "89/200-Train Acc:0.24679928414096916, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.899796485900879\n",
      "60/227 - Running loss: 1.915421485900879\n",
      "90/227 - Running loss: 1.884171485900879\n",
      "120/227 - Running loss: 1.9544838666915894\n",
      "150/227 - Running loss: 1.946671485900879\n",
      "180/227 - Running loss: 1.915421485900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "90/200-Train Loss:1.918587781784293, Val Loss:1.923234121552829\n",
      "90/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.946671485900879\n",
      "30/227 - Running loss: 1.954483985900879\n",
      "60/227 - Running loss: 1.915421485900879\n",
      "90/227 - Running loss: 1.891983985900879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/227 - Running loss: 1.923233985900879\n",
      "150/227 - Running loss: 1.915421485900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.899796485900879\n",
      "91/200-Train Loss:1.918656615433714, Val Loss:1.923234121552829\n",
      "91/200-Train Acc:0.2467648678414097, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.891983985900879\n",
      "30/227 - Running loss: 1.891983985900879\n",
      "60/227 - Running loss: 1.884171485900879\n",
      "90/227 - Running loss: 1.915421485900879\n",
      "120/227 - Running loss: 1.954483985900879\n",
      "150/227 - Running loss: 1.891983985900879\n",
      "180/227 - Running loss: 1.977921485900879\n",
      "210/227 - Running loss: 1.899796485900879\n",
      "92/200-Train Loss:1.9187254469825308, Val Loss:1.923234121552829\n",
      "92/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.954483985900879\n",
      "30/227 - Running loss: 1.907608985900879\n",
      "60/227 - Running loss: 1.876358985900879\n",
      "90/227 - Running loss: 1.907608985900879\n",
      "120/227 - Running loss: 1.923233985900879\n",
      "150/227 - Running loss: 1.876358985900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.876358985900879\n",
      "93/200-Train Loss:1.9186566143834118, Val Loss:1.923234121552829\n",
      "93/200-Train Acc:0.2467648678414097, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.977921485900879\n",
      "30/227 - Running loss: 1.938858985900879\n",
      "60/227 - Running loss: 1.907608985900879\n",
      "90/227 - Running loss: 1.884171485900879\n",
      "120/227 - Running loss: 1.876358985900879\n",
      "150/227 - Running loss: 2.024796485900879\n",
      "180/227 - Running loss: 1.907608985900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "94/200-Train Loss:1.9185877807339908, Val Loss:1.923234121552829\n",
      "94/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.954483985900879\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.946671485900879\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.884171485900879\n",
      "95/200-Train Loss:1.918691028582367, Val Loss:1.923234121552829\n",
      "95/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.899796485900879\n",
      "60/227 - Running loss: 1.938858985900879\n",
      "90/227 - Running loss: 1.899796485900879\n",
      "120/227 - Running loss: 1.962296485900879\n",
      "150/227 - Running loss: 1.891983985900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "96/200-Train Loss:1.9186910306829712, Val Loss:1.923234121552829\n",
      "96/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.938858985900879\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.931046485900879\n",
      "180/227 - Running loss: 1.938858985900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "97/200-Train Loss:1.9186221965083992, Val Loss:1.923234121552829\n",
      "97/200-Train Acc:0.24679928414096916, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.946671485900879\n",
      "30/227 - Running loss: 1.876358985900879\n",
      "60/227 - Running loss: 1.946671485900879\n",
      "90/227 - Running loss: 1.829483985900879\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.884171485900879\n",
      "180/227 - Running loss: 1.9622963666915894\n",
      "210/227 - Running loss: 1.884171485900879\n",
      "98/200-Train Loss:1.9185533644344313, Val Loss:1.923234121552829\n",
      "98/200-Train Acc:0.2468681167400881, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.899796485900879\n",
      "30/227 - Running loss: 1.962296485900879\n",
      "60/227 - Running loss: 1.946671485900879\n",
      "90/227 - Running loss: 1.946671485900879\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 1.977921485900879\n",
      "210/227 - Running loss: 1.884171485900879\n",
      "99/200-Train Loss:1.9187254459322287, Val Loss:1.923234121552829\n",
      "99/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.899796485900879\n",
      "30/227 - Running loss: 1.938858985900879\n",
      "60/227 - Running loss: 1.915421485900879\n",
      "90/227 - Running loss: 1.860733985900879\n",
      "120/227 - Running loss: 1.946671485900879\n",
      "150/227 - Running loss: 1.915421485900879\n",
      "180/227 - Running loss: 1.946671485900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "100/200-Train Loss:1.9187942780061966, Val Loss:1.923234121552829\n",
      "100/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.899796485900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.962296485900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.899796485900879\n",
      "150/227 - Running loss: 1.852921485900879\n",
      "180/227 - Running loss: 1.876358985900879\n",
      "210/227 - Running loss: 1.946671485900879\n",
      "101/200-Train Loss:1.918691027006914, Val Loss:1.923234121552829\n",
      "101/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.938858985900879\n",
      "30/227 - Running loss: 1.891983985900879\n",
      "60/227 - Running loss: 1.899796485900879\n",
      "90/227 - Running loss: 1.915421485900879\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.852921485900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "102/200-Train Loss:1.9186566107073544, Val Loss:1.923234121552829\n",
      "102/200-Train Acc:0.2467648678414097, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.907608985900879\n",
      "30/227 - Running loss: 1.876358985900879\n",
      "60/227 - Running loss: 1.9388588666915894\n",
      "90/227 - Running loss: 1.860733985900879\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 1.931046485900879\n",
      "180/227 - Running loss: 1.907608985900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "103/200-Train Loss:1.918691028582367, Val Loss:1.923234121552829\n",
      "103/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.899796485900879\n",
      "30/227 - Running loss: 1.884171485900879\n",
      "60/227 - Running loss: 1.970108985900879\n",
      "90/227 - Running loss: 1.938858985900879\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.884171485900879\n",
      "180/227 - Running loss: 1.970108985900879\n",
      "210/227 - Running loss: 1.9388588666915894\n",
      "104/200-Train Loss:1.918828695881209, Val Loss:1.923234121552829\n",
      "104/200-Train Acc:0.24659278634361234, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.860733985900879\n",
      "30/227 - Running loss: 1.954483985900879\n",
      "60/227 - Running loss: 1.938858985900879\n",
      "90/227 - Running loss: 1.931046485900879\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.954483985900879\n",
      "210/227 - Running loss: 1.899796485900879\n",
      "105/200-Train Loss:1.918759862756939, Val Loss:1.923234121552829\n",
      "105/200-Train Acc:0.24666161894273128, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.868546485900879\n",
      "30/227 - Running loss: 1.907608985900879\n",
      "60/227 - Running loss: 1.8216716051101685\n",
      "90/227 - Running loss: 1.876358985900879\n",
      "120/227 - Running loss: 1.860733985900879\n",
      "150/227 - Running loss: 1.915421485900879\n",
      "180/227 - Running loss: 1.852921485900879\n",
      "210/227 - Running loss: 1.884171485900879\n",
      "106/200-Train Loss:1.9186910296326691, Val Loss:1.923234121552829\n",
      "106/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.891983985900879\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.954483985900879\n",
      "90/227 - Running loss: 1.907608985900879\n",
      "120/227 - Running loss: 1.970108985900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.938858985900879\n",
      "210/227 - Running loss: 1.845108985900879\n",
      "107/200-Train Loss:1.918450116586055, Val Loss:1.923234121552829\n",
      "107/200-Train Acc:0.2469713656387665, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.915421485900879\n",
      "60/227 - Running loss: 1.938858985900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.962296485900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 1.899796485900879\n",
      "210/227 - Running loss: 1.931046485900879\n",
      "108/200-Train Loss:1.918759862231788, Val Loss:1.923234121552829\n",
      "108/200-Train Acc:0.24666161894273128, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.899796485900879\n",
      "30/227 - Running loss: 1.915421485900879\n",
      "60/227 - Running loss: 1.891983985900879\n",
      "90/227 - Running loss: 1.954483985900879\n",
      "120/227 - Running loss: 1.931046485900879\n",
      "150/227 - Running loss: 1.931046485900879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/227 - Running loss: 1.931046485900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "109/200-Train Loss:1.918587782309444, Val Loss:1.923234121552829\n",
      "109/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.884171485900879\n",
      "30/227 - Running loss: 1.962296485900879\n",
      "60/227 - Running loss: 1.946671485900879\n",
      "90/227 - Running loss: 1.868546485900879\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.915421485900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.931046485900879\n",
      "110/200-Train Loss:1.918518950235476, Val Loss:1.923234121552829\n",
      "110/200-Train Acc:0.24690253303964757, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.845108985900879\n",
      "90/227 - Running loss: 1.977921485900879\n",
      "120/227 - Running loss: 1.946671485900879\n",
      "150/227 - Running loss: 1.837296485900879\n",
      "180/227 - Running loss: 1.899796485900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "111/200-Train Loss:1.9184845302598592, Val Loss:1.923234121552829\n",
      "111/200-Train Acc:0.24693694933920704, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.962296485900879\n",
      "60/227 - Running loss: 1.954483985900879\n",
      "90/227 - Running loss: 1.8294841051101685\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.946671485900879\n",
      "180/227 - Running loss: 1.9310463666915894\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "112/200-Train Loss:1.9186221980838523, Val Loss:1.923234121552829\n",
      "112/200-Train Acc:0.24679928414096916, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.868546485900879\n",
      "30/227 - Running loss: 1.9857338666915894\n",
      "60/227 - Running loss: 1.970108985900879\n",
      "90/227 - Running loss: 1.954483985900879\n",
      "120/227 - Running loss: 1.829483985900879\n",
      "150/227 - Running loss: 1.962296485900879\n",
      "180/227 - Running loss: 1.891983985900879\n",
      "210/227 - Running loss: 1.962296485900879\n",
      "113/200-Train Loss:1.9187598638072414, Val Loss:1.923234121552829\n",
      "113/200-Train Acc:0.24666161894273128, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.868546485900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.860733985900879\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 1.837296485900879\n",
      "180/227 - Running loss: 1.876358985900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "114/200-Train Loss:1.9186221975587012, Val Loss:1.923234121552829\n",
      "114/200-Train Acc:0.24679928414096916, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.907608985900879\n",
      "60/227 - Running loss: 1.891983985900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 2.001358985900879\n",
      "150/227 - Running loss: 1.970108985900879\n",
      "180/227 - Running loss: 1.9544838666915894\n",
      "210/227 - Running loss: 1.868546485900879\n",
      "115/200-Train Loss:1.918622195458097, Val Loss:1.923234121552829\n",
      "115/200-Train Acc:0.24679928414096916, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.852921485900879\n",
      "30/227 - Running loss: 1.954483985900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.915421485900879\n",
      "120/227 - Running loss: 1.946671485900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.9857338666915894\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "116/200-Train Loss:1.918691027532065, Val Loss:1.923234121552829\n",
      "116/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.8529216051101685\n",
      "60/227 - Running loss: 1.938858985900879\n",
      "90/227 - Running loss: 1.860733985900879\n",
      "120/227 - Running loss: 1.899796485900879\n",
      "150/227 - Running loss: 1.852921485900879\n",
      "180/227 - Running loss: 1.970108985900879\n",
      "210/227 - Running loss: 1.860733985900879\n",
      "117/200-Train Loss:1.9187254459322287, Val Loss:1.923234121552829\n",
      "117/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.899796485900879\n",
      "60/227 - Running loss: 1.860733985900879\n",
      "90/227 - Running loss: 1.915421485900879\n",
      "120/227 - Running loss: 1.954483985900879\n",
      "150/227 - Running loss: 1.9232338666915894\n",
      "180/227 - Running loss: 1.915421485900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "118/200-Train Loss:1.9187254454070777, Val Loss:1.923234121552829\n",
      "118/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.876358985900879\n",
      "60/227 - Running loss: 1.9232338666915894\n",
      "90/227 - Running loss: 1.931046485900879\n",
      "120/227 - Running loss: 1.899796485900879\n",
      "150/227 - Running loss: 1.970108985900879\n",
      "180/227 - Running loss: 1.884171485900879\n",
      "210/227 - Running loss: 1.9466713666915894\n",
      "119/200-Train Loss:1.9187598638072414, Val Loss:1.923234121552829\n",
      "119/200-Train Acc:0.24666161894273128, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.868546485900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.884171485900879\n",
      "90/227 - Running loss: 1.876358985900879\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.946671485900879\n",
      "180/227 - Running loss: 1.907608985900879\n",
      "210/227 - Running loss: 1.946671485900879\n",
      "120/200-Train Loss:1.9186566128079587, Val Loss:1.923234121552829\n",
      "120/200-Train Acc:0.2467648678414097, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.946671485900879\n",
      "30/227 - Running loss: 1.884171485900879\n",
      "60/227 - Running loss: 1.938858985900879\n",
      "90/227 - Running loss: 1.907608985900879\n",
      "120/227 - Running loss: 1.899796485900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.899796485900879\n",
      "210/227 - Running loss: 1.876358985900879\n",
      "121/200-Train Loss:1.9187254459322287, Val Loss:1.923234121552829\n",
      "121/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.9857338666915894\n",
      "60/227 - Running loss: 1.938858985900879\n",
      "90/227 - Running loss: 1.931046485900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.946671485900879\n",
      "122/200-Train Loss:1.9184501144854509, Val Loss:1.923234121552829\n",
      "122/200-Train Acc:0.2469713656387665, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.9701088666915894\n",
      "30/227 - Running loss: 1.970108985900879\n",
      "60/227 - Running loss: 1.970108985900879\n",
      "90/227 - Running loss: 1.884171485900879\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.931046485900879\n",
      "180/227 - Running loss: 1.946671485900879\n",
      "210/227 - Running loss: 1.946671485900879\n",
      "123/200-Train Loss:1.9185877786333865, Val Loss:1.923234121552829\n",
      "123/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.923233985900879\n",
      "60/227 - Running loss: 1.962296485900879\n",
      "90/227 - Running loss: 1.876358985900879\n",
      "120/227 - Running loss: 1.9701088666915894\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.9857338666915894\n",
      "210/227 - Running loss: 1.931046485900879\n",
      "124/200-Train Loss:1.918553360758374, Val Loss:1.923234121552829\n",
      "124/200-Train Acc:0.2468681167400881, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.907608985900879\n",
      "60/227 - Running loss: 1.938858985900879\n",
      "90/227 - Running loss: 1.891983985900879\n",
      "120/227 - Running loss: 1.931046485900879\n",
      "150/227 - Running loss: 1.899796485900879\n",
      "180/227 - Running loss: 1.891983985900879\n",
      "210/227 - Running loss: 1.868546485900879\n",
      "125/200-Train Loss:1.9183124513878171, Val Loss:1.923234121552829\n",
      "125/200-Train Acc:0.24710903083700442, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.9779213666915894\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.923233985900879\n",
      "150/227 - Running loss: 1.899796485900879\n",
      "180/227 - Running loss: 1.931046485900879\n",
      "210/227 - Running loss: 1.977921485900879\n",
      "126/200-Train Loss:1.9185877802088398, Val Loss:1.923234121552829\n",
      "126/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.860733985900879\n",
      "30/227 - Running loss: 1.899796485900879\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.954483985900879\n",
      "120/227 - Running loss: 1.9544838666915894\n",
      "150/227 - Running loss: 1.9935463666915894\n",
      "180/227 - Running loss: 1.915421485900879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210/227 - Running loss: 1.938858985900879\n",
      "127/200-Train Loss:1.9191728573013507, Val Loss:1.923234121552829\n",
      "127/200-Train Acc:0.24624862334801761, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.977921485900879\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.813858985900879\n",
      "90/227 - Running loss: 1.915421485900879\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.962296485900879\n",
      "180/227 - Running loss: 1.970108985900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "128/200-Train Loss:1.918828694830907, Val Loss:1.923234121552829\n",
      "128/200-Train Acc:0.24659278634361234, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.946671485900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.946671485900879\n",
      "150/227 - Running loss: 1.9466713666915894\n",
      "180/227 - Running loss: 1.915421485900879\n",
      "210/227 - Running loss: 1.946671485900879\n",
      "129/200-Train Loss:1.9187254480328328, Val Loss:1.923234121552829\n",
      "129/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.891983985900879\n",
      "30/227 - Running loss: 1.923233985900879\n",
      "60/227 - Running loss: 1.938858985900879\n",
      "90/227 - Running loss: 1.899796485900879\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.946671485900879\n",
      "180/227 - Running loss: 1.915421485900879\n",
      "210/227 - Running loss: 1.9779213666915894\n",
      "130/200-Train Loss:1.9184845323604633, Val Loss:1.923234121552829\n",
      "130/200-Train Acc:0.24693694933920704, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.993546485900879\n",
      "30/227 - Running loss: 1.9388588666915894\n",
      "60/227 - Running loss: 1.845108985900879\n",
      "90/227 - Running loss: 1.915421485900879\n",
      "120/227 - Running loss: 1.931046485900879\n",
      "150/227 - Running loss: 1.993546485900879\n",
      "180/227 - Running loss: 1.907608985900879\n",
      "210/227 - Running loss: 1.938858985900879\n",
      "131/200-Train Loss:1.9185877807339908, Val Loss:1.923234121552829\n",
      "131/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.970108985900879\n",
      "60/227 - Running loss: 1.891983985900879\n",
      "90/227 - Running loss: 1.907608985900879\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.954483985900879\n",
      "180/227 - Running loss: 1.931046485900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "132/200-Train Loss:1.9187942759055923, Val Loss:1.923234121552829\n",
      "132/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.845108985900879\n",
      "60/227 - Running loss: 1.907608985900879\n",
      "90/227 - Running loss: 1.860733985900879\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.876358985900879\n",
      "180/227 - Running loss: 1.845108985900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "133/200-Train Loss:1.9187598638072414, Val Loss:1.923234121552829\n",
      "133/200-Train Acc:0.24666161894273128, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.938858985900879\n",
      "30/227 - Running loss: 1.891983985900879\n",
      "60/227 - Running loss: 1.938858985900879\n",
      "90/227 - Running loss: 1.946671485900879\n",
      "120/227 - Running loss: 1.852921485900879\n",
      "150/227 - Running loss: 1.8060466051101685\n",
      "180/227 - Running loss: 1.985733985900879\n",
      "210/227 - Running loss: 1.938858985900879\n",
      "134/200-Train Loss:1.9187598585557308, Val Loss:1.923234121552829\n",
      "134/200-Train Acc:0.24666161894273128, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.946671485900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.931046485900879\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.915421485900879\n",
      "180/227 - Running loss: 1.884171485900879\n",
      "210/227 - Running loss: 1.798233985900879\n",
      "135/200-Train Loss:1.9186910301578202, Val Loss:1.923234121552829\n",
      "135/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.938858985900879\n",
      "60/227 - Running loss: 1.8529216051101685\n",
      "90/227 - Running loss: 1.993546485900879\n",
      "120/227 - Running loss: 1.852921485900879\n",
      "150/227 - Running loss: 1.946671485900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "136/200-Train Loss:1.918828695356058, Val Loss:1.923234121552829\n",
      "136/200-Train Acc:0.24659278634361234, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.954483985900879\n",
      "30/227 - Running loss: 1.977921485900879\n",
      "60/227 - Running loss: 1.899796485900879\n",
      "90/227 - Running loss: 2.016983985900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.938858985900879\n",
      "180/227 - Running loss: 1.876358985900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "137/200-Train Loss:1.9187942806319518, Val Loss:1.923234121552829\n",
      "137/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.891983985900879\n",
      "30/227 - Running loss: 1.907608985900879\n",
      "60/227 - Running loss: 1.946671485900879\n",
      "90/227 - Running loss: 1.891983985900879\n",
      "120/227 - Running loss: 2.001358985900879\n",
      "150/227 - Running loss: 1.884171485900879\n",
      "180/227 - Running loss: 1.899796485900879\n",
      "210/227 - Running loss: 1.962296485900879\n",
      "138/200-Train Loss:1.9186566128079587, Val Loss:1.923234121552829\n",
      "138/200-Train Acc:0.2467648678414097, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.9622963666915894\n",
      "30/227 - Running loss: 1.985733985900879\n",
      "60/227 - Running loss: 1.985733985900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.884171485900879\n",
      "150/227 - Running loss: 1.876358985900879\n",
      "180/227 - Running loss: 1.954483985900879\n",
      "210/227 - Running loss: 1.931046485900879\n",
      "139/200-Train Loss:1.9184156997613444, Val Loss:1.923234121552829\n",
      "139/200-Train Acc:0.24700578193832598, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.813858985900879\n",
      "30/227 - Running loss: 1.884171485900879\n",
      "60/227 - Running loss: 1.891983985900879\n",
      "90/227 - Running loss: 1.946671485900879\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.899796485900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "140/200-Train Loss:1.9191040247022317, Val Loss:1.923234121552829\n",
      "140/200-Train Acc:0.24631745594713655, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.985733985900879\n",
      "30/227 - Running loss: 1.891983985900879\n",
      "60/227 - Running loss: 1.954483985900879\n",
      "90/227 - Running loss: 1.891983985900879\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 1.938858985900879\n",
      "180/227 - Running loss: 1.884171485900879\n",
      "210/227 - Running loss: 1.899796485900879\n",
      "141/200-Train Loss:1.918553360758374, Val Loss:1.923234121552829\n",
      "141/200-Train Acc:0.2468681167400881, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.907608985900879\n",
      "30/227 - Running loss: 1.938858985900879\n",
      "60/227 - Running loss: 1.9701088666915894\n",
      "90/227 - Running loss: 1.931046485900879\n",
      "120/227 - Running loss: 1.860733985900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 1.977921485900879\n",
      "210/227 - Running loss: 1.9779213666915894\n",
      "142/200-Train Loss:1.9189319421541324, Val Loss:1.923234121552829\n",
      "142/200-Train Acc:0.24648953744493393, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.884171485900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.938858985900879\n",
      "120/227 - Running loss: 1.884171485900879\n",
      "150/227 - Running loss: 1.884171485900879\n",
      "180/227 - Running loss: 1.907608985900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "143/200-Train Loss:1.9186910312081225, Val Loss:1.923234121552829\n",
      "143/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.829483985900879\n",
      "30/227 - Running loss: 1.852921485900879\n",
      "60/227 - Running loss: 1.821671485900879\n",
      "90/227 - Running loss: 1.891983985900879\n",
      "120/227 - Running loss: 1.884171485900879\n",
      "150/227 - Running loss: 1.970108985900879\n",
      "180/227 - Running loss: 1.9310463666915894\n",
      "210/227 - Running loss: 1.946671485900879\n",
      "144/200-Train Loss:1.9187254459322287, Val Loss:1.923234121552829\n",
      "144/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 2.016983985900879\n",
      "30/227 - Running loss: 1.954483985900879\n",
      "60/227 - Running loss: 1.962296485900879\n",
      "90/227 - Running loss: 1.9388588666915894\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.931046485900879\n",
      "180/227 - Running loss: 1.962296485900879\n",
      "210/227 - Running loss: 1.954483985900879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/200-Train Loss:1.918759861706637, Val Loss:1.923234121552829\n",
      "145/200-Train Acc:0.24666161894273128, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.899796485900879\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.891983985900879\n",
      "90/227 - Running loss: 1.868546485900879\n",
      "120/227 - Running loss: 1.970108985900879\n",
      "150/227 - Running loss: 1.891983985900879\n",
      "180/227 - Running loss: 1.907608985900879\n",
      "210/227 - Running loss: 1.868546485900879\n",
      "146/200-Train Loss:1.9186221986090035, Val Loss:1.923234121552829\n",
      "146/200-Train Acc:0.24679928414096916, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.876358985900879\n",
      "30/227 - Running loss: 1.868546485900879\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.891983985900879\n",
      "120/227 - Running loss: 1.923233985900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.860733985900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "147/200-Train Loss:1.9187254448819266, Val Loss:1.923234121552829\n",
      "147/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.9622963666915894\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.907608985900879\n",
      "90/227 - Running loss: 1.954483985900879\n",
      "120/227 - Running loss: 1.946671485900879\n",
      "150/227 - Running loss: 1.954483985900879\n",
      "180/227 - Running loss: 1.931046485900879\n",
      "210/227 - Running loss: 1.938858985900879\n",
      "148/200-Train Loss:1.9187942785313477, Val Loss:1.923234121552829\n",
      "148/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.876358985900879\n",
      "30/227 - Running loss: 1.899796485900879\n",
      "60/227 - Running loss: 1.852921485900879\n",
      "90/227 - Running loss: 1.985733985900879\n",
      "120/227 - Running loss: 1.829483985900879\n",
      "150/227 - Running loss: 1.868546485900879\n",
      "180/227 - Running loss: 1.931046485900879\n",
      "210/227 - Running loss: 1.962296485900879\n",
      "149/200-Train Loss:1.918759860131184, Val Loss:1.923234121552829\n",
      "149/200-Train Acc:0.24666161894273128, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.938858985900879\n",
      "30/227 - Running loss: 1.938858985900879\n",
      "60/227 - Running loss: 1.8997963666915894\n",
      "90/227 - Running loss: 1.954483985900879\n",
      "120/227 - Running loss: 1.923233985900879\n",
      "150/227 - Running loss: 1.962296485900879\n",
      "180/227 - Running loss: 1.962296485900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "150/200-Train Loss:1.9190007763287045, Val Loss:1.923234121552829\n",
      "150/200-Train Acc:0.246420704845815, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.977921485900879\n",
      "30/227 - Running loss: 1.946671485900879\n",
      "60/227 - Running loss: 1.868546485900879\n",
      "90/227 - Running loss: 1.9701088666915894\n",
      "120/227 - Running loss: 1.954483985900879\n",
      "150/227 - Running loss: 1.891983985900879\n",
      "180/227 - Running loss: 1.837296485900879\n",
      "210/227 - Running loss: 1.962296485900879\n",
      "151/200-Train Loss:1.9185877796836888, Val Loss:1.923234121552829\n",
      "151/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.946671485900879\n",
      "30/227 - Running loss: 1.938858985900879\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.946671485900879\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.876358985900879\n",
      "180/227 - Running loss: 2.040421485900879\n",
      "210/227 - Running loss: 1.946671485900879\n",
      "152/200-Train Loss:1.9185189518109291, Val Loss:1.923234121552829\n",
      "152/200-Train Acc:0.24690253303964757, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.977921485900879\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.946671485900879\n",
      "90/227 - Running loss: 1.907608985900879\n",
      "120/227 - Running loss: 1.954483985900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.899796485900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "153/200-Train Loss:1.9188631137562218, Val Loss:1.923234121552829\n",
      "153/200-Train Acc:0.24655837004405287, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.907608985900879\n",
      "60/227 - Running loss: 1.946671485900879\n",
      "90/227 - Running loss: 1.962296485900879\n",
      "120/227 - Running loss: 1.9701088666915894\n",
      "150/227 - Running loss: 1.891983985900879\n",
      "180/227 - Running loss: 1.915421485900879\n",
      "210/227 - Running loss: 1.899796485900879\n",
      "154/200-Train Loss:1.918518948660023, Val Loss:1.923234121552829\n",
      "154/200-Train Acc:0.24690253303964757, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.899796485900879\n",
      "30/227 - Running loss: 2.009171485900879\n",
      "60/227 - Running loss: 1.907608985900879\n",
      "90/227 - Running loss: 1.931046485900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.931046485900879\n",
      "180/227 - Running loss: 1.891983985900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "155/200-Train Loss:1.9188975295306303, Val Loss:1.923234121552829\n",
      "155/200-Train Acc:0.2465239537444934, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.884171485900879\n",
      "30/227 - Running loss: 1.837296485900879\n",
      "60/227 - Running loss: 1.946671485900879\n",
      "90/227 - Running loss: 1.9857338666915894\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.891983985900879\n",
      "180/227 - Running loss: 1.899796485900879\n",
      "210/227 - Running loss: 1.962296485900879\n",
      "156/200-Train Loss:1.9187254459322287, Val Loss:1.923234121552829\n",
      "156/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.9388588666915894\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.860733985900879\n",
      "120/227 - Running loss: 1.954483985900879\n",
      "150/227 - Running loss: 1.9857338666915894\n",
      "180/227 - Running loss: 1.954483985900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "157/200-Train Loss:1.9185189470845698, Val Loss:1.923234121552829\n",
      "157/200-Train Acc:0.24690253303964757, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.907608985900879\n",
      "30/227 - Running loss: 1.884171485900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.938858985900879\n",
      "120/227 - Running loss: 1.884171485900879\n",
      "150/227 - Running loss: 1.899796485900879\n",
      "180/227 - Running loss: 1.938858985900879\n",
      "210/227 - Running loss: 1.852921485900879\n",
      "158/200-Train Loss:1.9187254433064733, Val Loss:1.923234121552829\n",
      "158/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.9466713666915894\n",
      "30/227 - Running loss: 1.821671485900879\n",
      "60/227 - Running loss: 1.899796485900879\n",
      "90/227 - Running loss: 1.907608985900879\n",
      "120/227 - Running loss: 1.985733985900879\n",
      "150/227 - Running loss: 1.891983985900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.899796485900879\n",
      "159/200-Train Loss:1.9185877796836888, Val Loss:1.923234121552829\n",
      "159/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.985733985900879\n",
      "30/227 - Running loss: 1.907608985900879\n",
      "60/227 - Running loss: 1.915421485900879\n",
      "90/227 - Running loss: 1.954483985900879\n",
      "120/227 - Running loss: 1.962296485900879\n",
      "150/227 - Running loss: 1.852921485900879\n",
      "180/227 - Running loss: 1.915421485900879\n",
      "210/227 - Running loss: 1.837296485900879\n",
      "160/200-Train Loss:1.9184156987110423, Val Loss:1.923234121552829\n",
      "160/200-Train Acc:0.24700578193832598, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.845108985900879\n",
      "60/227 - Running loss: 1.962296485900879\n",
      "90/227 - Running loss: 1.884171485900879\n",
      "120/227 - Running loss: 1.977921485900879\n",
      "150/227 - Running loss: 1.931046485900879\n",
      "180/227 - Running loss: 1.891983985900879\n",
      "210/227 - Running loss: 1.954483985900879\n",
      "161/200-Train Loss:1.918656616484016, Val Loss:1.923234121552829\n",
      "161/200-Train Acc:0.2467648678414097, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.884171485900879\n",
      "30/227 - Running loss: 1.993546485900879\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.845108985900879\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.852921485900879\n",
      "210/227 - Running loss: 1.931046485900879\n",
      "162/200-Train Loss:1.9188975290054793, Val Loss:1.923234121552829\n",
      "162/200-Train Acc:0.2465239537444934, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.931046485900879\n",
      "30/227 - Running loss: 1.884171485900879\n",
      "60/227 - Running loss: 1.962296485900879\n",
      "90/227 - Running loss: 1.899796485900879\n",
      "120/227 - Running loss: 1.923233985900879\n",
      "150/227 - Running loss: 1.899796485900879\n",
      "180/227 - Running loss: 2.009171485900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "163/200-Train Loss:1.918759860131184, Val Loss:1.923234121552829\n",
      "163/200-Train Acc:0.24666161894273128, Val Acc:0.2421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/227 - Running loss: 1.868546485900879\n",
      "30/227 - Running loss: 1.899796485900879\n",
      "60/227 - Running loss: 1.9388588666915894\n",
      "90/227 - Running loss: 1.954483985900879\n",
      "120/227 - Running loss: 1.884171485900879\n",
      "150/227 - Running loss: 1.970108985900879\n",
      "180/227 - Running loss: 1.8138591051101685\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "164/200-Train Loss:1.9189319421541324, Val Loss:1.923234121552829\n",
      "164/200-Train Acc:0.24648953744493393, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.946671485900879\n",
      "30/227 - Running loss: 1.891983985900879\n",
      "60/227 - Running loss: 1.915421485900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.907608985900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "165/200-Train Loss:1.9185877791585375, Val Loss:1.923234121552829\n",
      "165/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.876358985900879\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.915421485900879\n",
      "120/227 - Running loss: 1.931046485900879\n",
      "150/227 - Running loss: 1.931046485900879\n",
      "180/227 - Running loss: 1.9701088666915894\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "166/200-Train Loss:1.9187942764307433, Val Loss:1.923234121552829\n",
      "166/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.993546485900879\n",
      "30/227 - Running loss: 1.899796485900879\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.938858985900879\n",
      "120/227 - Running loss: 1.876358985900879\n",
      "150/227 - Running loss: 1.954483985900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.884171485900879\n",
      "167/200-Train Loss:1.918381283461785, Val Loss:1.923234121552829\n",
      "167/200-Train Acc:0.24704019823788545, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.938858985900879\n",
      "30/227 - Running loss: 1.907608985900879\n",
      "60/227 - Running loss: 1.907608985900879\n",
      "90/227 - Running loss: 1.891983985900879\n",
      "120/227 - Running loss: 1.962296485900879\n",
      "150/227 - Running loss: 1.860733985900879\n",
      "180/227 - Running loss: 1.946671485900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "168/200-Train Loss:1.9186566143834118, Val Loss:1.923234121552829\n",
      "168/200-Train Acc:0.2467648678414097, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.962296485900879\n",
      "30/227 - Running loss: 1.915421485900879\n",
      "60/227 - Running loss: 1.9857338666915894\n",
      "90/227 - Running loss: 1.915421485900879\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 1.954483985900879\n",
      "180/227 - Running loss: 1.962296485900879\n",
      "210/227 - Running loss: 1.938858985900879\n",
      "169/200-Train Loss:1.9185533670601866, Val Loss:1.923234121552829\n",
      "169/200-Train Acc:0.2468681167400881, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.852921485900879\n",
      "60/227 - Running loss: 1.9935463666915894\n",
      "90/227 - Running loss: 1.884171485900879\n",
      "120/227 - Running loss: 1.915421485900879\n",
      "150/227 - Running loss: 1.915421485900879\n",
      "180/227 - Running loss: 1.868546485900879\n",
      "210/227 - Running loss: 1.938858985900879\n",
      "170/200-Train Loss:1.9183812808360297, Val Loss:1.923234121552829\n",
      "170/200-Train Acc:0.24704019823788545, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.868546485900879\n",
      "30/227 - Running loss: 2.001358985900879\n",
      "60/227 - Running loss: 1.837296485900879\n",
      "90/227 - Running loss: 1.954483985900879\n",
      "120/227 - Running loss: 1.876358985900879\n",
      "150/227 - Running loss: 1.946671485900879\n",
      "180/227 - Running loss: 1.938858985900879\n",
      "210/227 - Running loss: 1.946671485900879\n",
      "171/200-Train Loss:1.918759861706637, Val Loss:1.923234121552829\n",
      "171/200-Train Acc:0.24666161894273128, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.829483985900879\n",
      "30/227 - Running loss: 1.891983985900879\n",
      "60/227 - Running loss: 1.876358985900879\n",
      "90/227 - Running loss: 1.829483985900879\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 1.845108985900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "172/200-Train Loss:1.9187598632820901, Val Loss:1.923234121552829\n",
      "172/200-Train Acc:0.24666161894273128, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.891983985900879\n",
      "60/227 - Running loss: 1.891983985900879\n",
      "90/227 - Running loss: 1.899796485900879\n",
      "120/227 - Running loss: 1.884171485900879\n",
      "150/227 - Running loss: 1.946671485900879\n",
      "180/227 - Running loss: 1.884171485900879\n",
      "210/227 - Running loss: 1.876358985900879\n",
      "173/200-Train Loss:1.918484528159255, Val Loss:1.923234121552829\n",
      "173/200-Train Acc:0.24693694933920704, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.884171485900879\n",
      "30/227 - Running loss: 1.9701088666915894\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 2.001358985900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.931046485900879\n",
      "210/227 - Running loss: 1.977921485900879\n",
      "174/200-Train Loss:1.918691026481763, Val Loss:1.923234121552829\n",
      "174/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.946671485900879\n",
      "30/227 - Running loss: 1.923233985900879\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.915421485900879\n",
      "120/227 - Running loss: 1.985733985900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 1.954483985900879\n",
      "210/227 - Running loss: 1.931046485900879\n",
      "175/200-Train Loss:1.9185877802088398, Val Loss:1.923234121552829\n",
      "175/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.852921485900879\n",
      "30/227 - Running loss: 1.9857338666915894\n",
      "60/227 - Running loss: 1.9622963666915894\n",
      "90/227 - Running loss: 1.946671485900879\n",
      "120/227 - Running loss: 1.899796485900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 1.954483985900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "176/200-Train Loss:1.918691029107518, Val Loss:1.923234121552829\n",
      "176/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.962296485900879\n",
      "30/227 - Running loss: 1.915421485900879\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.9779213666915894\n",
      "120/227 - Running loss: 1.938858985900879\n",
      "150/227 - Running loss: 1.899796485900879\n",
      "180/227 - Running loss: 1.860733985900879\n",
      "210/227 - Running loss: 1.931046485900879\n",
      "177/200-Train Loss:1.9189663595039939, Val Loss:1.923234121552829\n",
      "177/200-Train Acc:0.24645512114537446, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.876358985900879\n",
      "120/227 - Running loss: 1.884171485900879\n",
      "150/227 - Running loss: 1.915421485900879\n",
      "180/227 - Running loss: 1.962296485900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "178/200-Train Loss:1.9186221959832481, Val Loss:1.923234121552829\n",
      "178/200-Train Acc:0.24679928414096916, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.9857338666915894\n",
      "60/227 - Running loss: 1.907608985900879\n",
      "90/227 - Running loss: 1.931046485900879\n",
      "120/227 - Running loss: 2.001358985900879\n",
      "150/227 - Running loss: 1.891983985900879\n",
      "180/227 - Running loss: 1.891983985900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "179/200-Train Loss:1.918691029107518, Val Loss:1.923234121552829\n",
      "179/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.899796485900879\n",
      "30/227 - Running loss: 1.938858985900879\n",
      "60/227 - Running loss: 1.985733985900879\n",
      "90/227 - Running loss: 1.891983985900879\n",
      "120/227 - Running loss: 1.946671485900879\n",
      "150/227 - Running loss: 1.962296485900879\n",
      "180/227 - Running loss: 2.001358985900879\n",
      "210/227 - Running loss: 1.962296485900879\n",
      "180/200-Train Loss:1.9186221965083992, Val Loss:1.923234121552829\n",
      "180/200-Train Acc:0.24679928414096916, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.884171485900879\n",
      "30/227 - Running loss: 1.962296485900879\n",
      "60/227 - Running loss: 1.891983985900879\n",
      "90/227 - Running loss: 1.907608985900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.876358985900879\n",
      "180/227 - Running loss: 1.954483985900879\n",
      "210/227 - Running loss: 1.899796485900879\n",
      "181/200-Train Loss:1.9185877796836888, Val Loss:1.923234121552829\n",
      "181/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.891983985900879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/227 - Running loss: 1.938858985900879\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.962296485900879\n",
      "120/227 - Running loss: 1.884171485900879\n",
      "150/227 - Running loss: 1.954483985900879\n",
      "180/227 - Running loss: 1.837296485900879\n",
      "210/227 - Running loss: 1.868546485900879\n",
      "182/200-Train Loss:1.9186910296326691, Val Loss:1.923234121552829\n",
      "182/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 2.009171485900879\n",
      "30/227 - Running loss: 1.915421485900879\n",
      "60/227 - Running loss: 1.868546485900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.884171485900879\n",
      "180/227 - Running loss: 1.884171485900879\n",
      "210/227 - Running loss: 1.876358985900879\n",
      "183/200-Train Loss:1.9185877786333865, Val Loss:1.923234121552829\n",
      "183/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.899796485900879\n",
      "90/227 - Running loss: 1.954483985900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.954483985900879\n",
      "180/227 - Running loss: 1.899796485900879\n",
      "210/227 - Running loss: 1.876358985900879\n",
      "184/200-Train Loss:1.9187254459322287, Val Loss:1.923234121552829\n",
      "184/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.962296485900879\n",
      "30/227 - Running loss: 1.923233985900879\n",
      "60/227 - Running loss: 1.915421485900879\n",
      "90/227 - Running loss: 1.876358985900879\n",
      "120/227 - Running loss: 1.899796485900879\n",
      "150/227 - Running loss: 1.829483985900879\n",
      "180/227 - Running loss: 1.860733985900879\n",
      "210/227 - Running loss: 1.868546485900879\n",
      "185/200-Train Loss:1.918450115535753, Val Loss:1.923234121552829\n",
      "185/200-Train Acc:0.2469713656387665, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.884171485900879\n",
      "30/227 - Running loss: 1.907608985900879\n",
      "60/227 - Running loss: 1.852921485900879\n",
      "90/227 - Running loss: 1.876358985900879\n",
      "120/227 - Running loss: 1.884171485900879\n",
      "150/227 - Running loss: 1.884171485900879\n",
      "180/227 - Running loss: 1.954483985900879\n",
      "210/227 - Running loss: 2.001358985900879\n",
      "186/200-Train Loss:1.9186910301578202, Val Loss:1.923234121552829\n",
      "186/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.977921485900879\n",
      "30/227 - Running loss: 1.931046485900879\n",
      "60/227 - Running loss: 1.931046485900879\n",
      "90/227 - Running loss: 1.899796485900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.931046485900879\n",
      "180/227 - Running loss: 1.954483985900879\n",
      "210/227 - Running loss: 1.954483985900879\n",
      "187/200-Train Loss:1.9188975263797239, Val Loss:1.923234121552829\n",
      "187/200-Train Acc:0.2465239537444934, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.907608985900879\n",
      "30/227 - Running loss: 1.876358985900879\n",
      "60/227 - Running loss: 1.962296485900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.899796485900879\n",
      "150/227 - Running loss: 1.9779213666915894\n",
      "180/227 - Running loss: 1.931046485900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "188/200-Train Loss:1.9190007779041576, Val Loss:1.923234121552829\n",
      "188/200-Train Acc:0.246420704845815, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.907608985900879\n",
      "30/227 - Running loss: 1.899796485900879\n",
      "60/227 - Running loss: 1.891983985900879\n",
      "90/227 - Running loss: 1.899796485900879\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.970108985900879\n",
      "210/227 - Running loss: 1.9779213666915894\n",
      "189/200-Train Loss:1.9186566133331098, Val Loss:1.923234121552829\n",
      "189/200-Train Acc:0.2467648678414097, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.915421485900879\n",
      "30/227 - Running loss: 1.923233985900879\n",
      "60/227 - Running loss: 1.891983985900879\n",
      "90/227 - Running loss: 1.899796485900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.946671485900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.899796485900879\n",
      "190/200-Train Loss:1.9185877796836888, Val Loss:1.923234121552829\n",
      "190/200-Train Acc:0.24683370044052863, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.884171485900879\n",
      "30/227 - Running loss: 1.852921485900879\n",
      "60/227 - Running loss: 2.009171485900879\n",
      "90/227 - Running loss: 1.907608985900879\n",
      "120/227 - Running loss: 1.852921485900879\n",
      "150/227 - Running loss: 1.938858985900879\n",
      "180/227 - Running loss: 1.923233985900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "191/200-Train Loss:1.9186910301578202, Val Loss:1.923234121552829\n",
      "191/200-Train Acc:0.24673045154185022, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.946671485900879\n",
      "30/227 - Running loss: 1.915421485900879\n",
      "60/227 - Running loss: 1.907608985900879\n",
      "90/227 - Running loss: 1.970108985900879\n",
      "120/227 - Running loss: 1.907608985900879\n",
      "150/227 - Running loss: 1.915421485900879\n",
      "180/227 - Running loss: 1.876358985900879\n",
      "210/227 - Running loss: 1.891983985900879\n",
      "192/200-Train Loss:1.9187942759055923, Val Loss:1.923234121552829\n",
      "192/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.9622963666915894\n",
      "30/227 - Running loss: 1.891983985900879\n",
      "60/227 - Running loss: 1.923233985900879\n",
      "90/227 - Running loss: 1.923233985900879\n",
      "120/227 - Running loss: 1.868546485900879\n",
      "150/227 - Running loss: 1.923233985900879\n",
      "180/227 - Running loss: 1.876358985900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "193/200-Train Loss:1.9185533639092802, Val Loss:1.923234121552829\n",
      "193/200-Train Acc:0.2468681167400881, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.891983985900879\n",
      "30/227 - Running loss: 1.954483985900879\n",
      "60/227 - Running loss: 1.868546485900879\n",
      "90/227 - Running loss: 1.884171485900879\n",
      "120/227 - Running loss: 1.899796485900879\n",
      "150/227 - Running loss: 1.985733985900879\n",
      "180/227 - Running loss: 1.915421485900879\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "194/200-Train Loss:1.9187598638072414, Val Loss:1.923234121552829\n",
      "194/200-Train Acc:0.24666161894273128, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.938858985900879\n",
      "30/227 - Running loss: 1.954483985900879\n",
      "60/227 - Running loss: 1.907608985900879\n",
      "90/227 - Running loss: 1.9622963666915894\n",
      "120/227 - Running loss: 1.931046485900879\n",
      "150/227 - Running loss: 1.899796485900879\n",
      "180/227 - Running loss: 1.954483985900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "195/200-Train Loss:1.918828693255454, Val Loss:1.923234121552829\n",
      "195/200-Train Acc:0.24659278634361234, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.923233985900879\n",
      "30/227 - Running loss: 1.962296485900879\n",
      "60/227 - Running loss: 1.860733985900879\n",
      "90/227 - Running loss: 1.891983985900879\n",
      "120/227 - Running loss: 1.899796485900879\n",
      "150/227 - Running loss: 1.915421485900879\n",
      "180/227 - Running loss: 1.9544838666915894\n",
      "210/227 - Running loss: 1.931046485900879\n",
      "196/200-Train Loss:1.9187254454070777, Val Loss:1.923234121552829\n",
      "196/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.9466713666915894\n",
      "30/227 - Running loss: 1.915421485900879\n",
      "60/227 - Running loss: 1.9544838666915894\n",
      "90/227 - Running loss: 1.876358985900879\n",
      "120/227 - Running loss: 1.891983985900879\n",
      "150/227 - Running loss: 1.852921485900879\n",
      "180/227 - Running loss: 1.8216716051101685\n",
      "210/227 - Running loss: 1.907608985900879\n",
      "197/200-Train Loss:1.9187254448819266, Val Loss:1.923234121552829\n",
      "197/200-Train Acc:0.24669603524229075, Val Acc:0.2421875\n",
      "0/227 - Running loss: 2.016983985900879\n",
      "30/227 - Running loss: 1.915421485900879\n",
      "60/227 - Running loss: 1.868546485900879\n",
      "90/227 - Running loss: 1.938858985900879\n",
      "120/227 - Running loss: 1.829483985900879\n",
      "150/227 - Running loss: 1.954483985900879\n",
      "180/227 - Running loss: 1.946671485900879\n",
      "210/227 - Running loss: 1.923233985900879\n",
      "198/200-Train Loss:1.9186221970335502, Val Loss:1.923234121552829\n",
      "198/200-Train Acc:0.24679928414096916, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.970108985900879\n",
      "30/227 - Running loss: 1.923233985900879\n",
      "60/227 - Running loss: 1.915421485900879\n",
      "90/227 - Running loss: 1.876358985900879\n",
      "120/227 - Running loss: 1.977921485900879\n",
      "150/227 - Running loss: 1.931046485900879\n",
      "180/227 - Running loss: 1.899796485900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "199/200-Train Loss:1.9187942774810456, Val Loss:1.923234121552829\n",
      "199/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "0/227 - Running loss: 1.907608985900879\n",
      "30/227 - Running loss: 1.915421485900879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/227 - Running loss: 1.876358985900879\n",
      "90/227 - Running loss: 1.946671485900879\n",
      "120/227 - Running loss: 1.954483985900879\n",
      "150/227 - Running loss: 1.907608985900879\n",
      "180/227 - Running loss: 1.907608985900879\n",
      "210/227 - Running loss: 1.915421485900879\n",
      "200/200-Train Loss:1.9187942769558943, Val Loss:1.923234121552829\n",
      "200/200-Train Acc:0.2466272026431718, Val Acc:0.2421875\n",
      "Bset Accuracy: 0.2421875\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "\n",
    "best_acc = 0.0\n",
    "train_loss,train_accuracy = [],[]\n",
    "val_loss,val_accuracy = [],[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ### 1 Epoch 수행\n",
    "    losses, accuracies = train_one_epoch(dataloaders, model, optimizer, loss_func, device)\n",
    "    \n",
    "    # LR Scheduling\n",
    "    scheduler.step(losses['val']) # 1 Epoch 수행 후 learning rate 조정\n",
    "    \n",
    "    # EarlyStopping\n",
    "    early_stopping(losses['val'], model) # 현재 과적합 상황 추적\n",
    "    if early_stopping.early_stop: # 조건 만족 시 조기 종료\n",
    "        break\n",
    "        \n",
    "    train_loss.append(losses['train'])\n",
    "    val_loss.append(losses['val'])\n",
    "    train_accuracy.append(accuracies['train'])\n",
    "    val_accuracy.append(accuracies['val'])\n",
    "    \n",
    "    print(f\"{epoch+1}/{num_epochs}-Train Loss:{losses['train']}, Val Loss:{losses['val']}\")\n",
    "    print(f\"{epoch+1}/{num_epochs}-Train Acc:{accuracies['train']}, Val Acc:{accuracies['val']}\")\n",
    "    \n",
    "    # deepcopy: 배열의 내부 객체까지 복사를 해서 사용\n",
    "    # copy: 배열의 내부 객체까지 깊은 복사를 해주지 않음\n",
    "    if (epoch > 3) and (accuracies['val'] > best_acc):\n",
    "        best_acc = accuracies['val']\n",
    "        best_model = copy.deepcopy(model.state_dict()) \n",
    "        save_best_model(best_model, f'model_{epoch+1:02d}.pth')\n",
    "\n",
    "print(f'Bset Accuracy: {best_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3656ccdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Loss, Accuracy 시각화\n",
    "\n",
    "plt.figure(figsize = (6,5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(211)\n",
    "plt.plot(train_loss,label = \"train\")\n",
    "plt.plot(val_loss,label = \"val\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.grid(\"on\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(212)\n",
    "plt.plot(train_accuracy,label = \"train\")\n",
    "plt.plot(val_accuracy,label = \"val\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.grid(\"on\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d090edb6",
   "metadata": {},
   "source": [
    "- learning rate가 높게 설정된 듯 하다.\n",
    "- validation loss가 전혀 갱신되지 못하고 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
