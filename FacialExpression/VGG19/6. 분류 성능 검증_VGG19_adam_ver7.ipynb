{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c28d2d10",
   "metadata": {},
   "source": [
    "# **필요한 요소들 준비하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ebaad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 필요한 라이브러리 준비\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import argparse\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from ipywidgets import interact\n",
    "from module.pytorchtools import EarlyStopping # 사용자 정의 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43f906",
   "metadata": {},
   "source": [
    "### **Random Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a19f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 가중치 함수\n",
    "\n",
    "def make_weights(df,classes):\n",
    "    weight_list = []   # 가중치를 저장하는 배열을 생성한다.\n",
    " \n",
    "    for c in classes:\n",
    "        count = len(df[df['emotion'] == str(c)]) #각 클래스의 데이터 수 카운트 \n",
    "        weight = 1 / count    \n",
    "        weights = [weight] * count    # 라벨이 뽑힐 가중치를 count의 역수로 동일하게 전체 라벨에 할당 \n",
    "        weight_list += weights\n",
    " \n",
    "    return weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "977eb1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터 가져오기\n",
    "train_data_dir = './DataSet/archive/train/'\n",
    "train_data_df = pd.read_csv(os.path.join(train_data_dir,'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb97b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 가중치 생성\n",
    "feelings_list = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "weights = make_weights(train_data_df,feelings_list)\n",
    "# print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2a4e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 가중 손실 함수\n",
    "\n",
    "def get_class_weights(df,classes):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # gpu 할당\n",
    "    cnt_lists = [] # 각 클래스의 데이터 수 저장\n",
    "    weight_list = []   # 가중치를 저장하는 배열을 생성한다.\n",
    " \n",
    "    for c in classes:\n",
    "        count = len(df[df['emotion'] == str(c)]) #각 클래스의 데이터 수 카운트 \n",
    "        cnt_lists.append(count)\n",
    "    \n",
    "    weights = [1-(x / sum(cnt_lists)) for x in cnt_lists]\n",
    "    class_weights = torch.cuda.FloatTensor(weights).to(device) # 가중치 설정\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "\n",
    "### 가중치 생성\n",
    "feelings_list = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "class_weights = get_class_weights(train_data_df,feelings_list)\n",
    "# print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f292aa",
   "metadata": {},
   "source": [
    "### **DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b9cc8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = './DataSet/archive/train/'\n",
    "train_data_df = pd.read_csv(os.path.join(train_data_dir,'train.csv'))\n",
    "\n",
    "# 분류에 사용할 class 정의(7개의 감정들)\n",
    "feelings_list = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6c76367",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b202728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 48\n",
    "\n",
    "### 이미지 파일 경로를 리스트 형태로 저장하기 위한 함수\n",
    "def list_image_file(data_dir,sub_dir):\n",
    "    image_files = []\n",
    "    \n",
    "    images_dir = os.path.join(data_dir,sub_dir)\n",
    "    for file_path in os.listdir(images_dir):\n",
    "        image_files.append(os.path.join(sub_dir,file_path))\n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70ba7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습 데이터셋 클래스\n",
    "class Feeling_dataset(Dataset):\n",
    "    ### 생성자\n",
    "    def __init__(self,data_dir,transform = None):\n",
    "        self.data_dir = data_dir # 데이터가 저장된 상위 directory\n",
    "        angry_imgs = list_image_file(data_dir,'angry')\n",
    "        disgust_imgs = list_image_file(data_dir,'disgust')\n",
    "        fear_imgs = list_image_file(data_dir,'fear')\n",
    "        happy_imgs = list_image_file(data_dir,'happy')\n",
    "        neutral_imgs = list_image_file(data_dir,'neutral')\n",
    "        sad_imgs = list_image_file(data_dir,'sad')\n",
    "        surprise_imgs = list_image_file(data_dir,'surprise')\n",
    "        \n",
    "        # 모든 사진들의 경로를 하나의 리스트에 저장\n",
    "        self.files_path = angry_imgs + disgust_imgs + fear_imgs + happy_imgs + neutral_imgs + sad_imgs + surprise_imgs\n",
    "        self.transform = transform\n",
    "    \n",
    "    ### 데이터 개수 확인\n",
    "    def __len__(self):\n",
    "        return len(self.files_path) # 전체 데이터 개수\n",
    "    \n",
    "    ### getitem\n",
    "    def __getitem__(self,index):\n",
    "        # image(feature data)\n",
    "        image_file = os.path.join(self.data_dir,self.files_path[index])\n",
    "        image = cv2.imread(image_file)\n",
    "        image = cv2.resize(image,dsize = (IMAGE_SIZE,IMAGE_SIZE),interpolation = cv2.INTER_LINEAR)\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # feeling(target data)\n",
    "        target = feelings_list.index(self.files_path[index].split(os.sep)[0])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image) # feature data에 대해서만 데이터 변형 수행\n",
    "            target = torch.Tensor([target]).long()\n",
    "            \n",
    "        return {'image':image,'target':target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c05ad0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformation\n",
    "\n",
    "# 학습 feature data 변환\n",
    "train_transformer = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(), # 수평으로 뒤집기\n",
    "    transforms.RandomVerticalFlip(), # 수직으로 뒤집기\n",
    "    transforms.ToTensor(), # 텐서로 변환\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) # 정규화\n",
    "])\n",
    "\n",
    "# 검증 feature data 변환\n",
    "val_transformer = transforms.Compose([\n",
    "    transforms.ToTensor(), # 텐서로 변환\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) # 정규화\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "814614ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터로더 구현\n",
    "def build_dataloader(train_data_dir,val_data_dir):\n",
    "    dataloaders = {}\n",
    "    \n",
    "    weights = make_weights(train_data_df,feelings_list)\n",
    "    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))\n",
    "    \n",
    "    train_dset = Feeling_dataset(train_data_dir,train_transformer)\n",
    "    dataloaders['train'] = DataLoader(train_dset,batch_size = 128,shuffle = True,drop_last = True)\n",
    "    \n",
    "    val_dset = Feeling_dataset(val_data_dir,val_transformer)\n",
    "    dataloaders['val'] = DataLoader(val_dset,batch_size = 128,shuffle = False,drop_last = False)\n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b685e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = './DataSet/archive/train/'\n",
    "val_data_dir = train_data_dir = './DataSet/archive/valid/'\n",
    "dataloaders = build_dataloader(train_data_dir,val_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7defd12e",
   "metadata": {},
   "source": [
    "### **Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9819493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61199fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델 생성 함수\n",
    "# 기존의 VGG19 모델 호출 -> head 부분 수정\n",
    "\n",
    "def build_vgg19_based_model():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = models.vgg19(pretrained = True) # 이미 학습된 vgg19 모델 불러오기\n",
    "    # 일반 NN Layer(FC Layer)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(output_size = (1,1)) # 각 구역의 평균값 출력\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Flatten(), # 평탄화\n",
    "        nn.Linear(512,256), # 512 -> 256\n",
    "        nn.ReLU(), # 활성화 함수\n",
    "        nn.Dropout(0.1), # 과적합 방지\n",
    "        nn.Linear(256,7), # 256 -> 7(7개의 감정으로 분류되니)\n",
    "        nn.Softmax() # 활성화 함수(각 클래스에 속할 확률 추정)\n",
    "    )\n",
    "    \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d9567c",
   "metadata": {},
   "source": [
    "### **Estimation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10f7d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델 검증을 위한 accuracy\n",
    "\n",
    "@torch.no_grad() \n",
    "def get_accuracy(image,target,model):\n",
    "    batch_size = image.shape[0] \n",
    "    prediction = model(image) # 예측 \n",
    "    _,pred_label = torch.max(prediction,dim = 1) # 예측이 어느 클래스에 속하는지 확률이 가장 높은 1개 선택\n",
    "    is_correct = (pred_label == target)\n",
    "    \n",
    "    return is_correct.cpu().numpy().sum() / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f7d6f",
   "metadata": {},
   "source": [
    "# **성능 평가**\n",
    "- 활용 모델(Base Model): VGG19 + Head 부분 수정\n",
    "- loss function: CrossEntropyLoss -- 가중치 적용\n",
    "- optimizer: Adam(lr = 1e-5)\n",
    "- lr_scheduler: ReduceLROnPlateau(patience = 5, factor = 0.1, min_lr = 1e-12)\n",
    "- batch size: 128\n",
    "- Epoch number: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0a02f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\users\\bin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # 장치 얻어오기\n",
    "\n",
    "### 경로 설정\n",
    "train_data_dir = './DataSet/archive/train/'\n",
    "val_data_dir = './DataSet/archive/valid/'\n",
    "\n",
    "### 필요한 요소들 준비\n",
    "dataloaders = build_dataloader(train_data_dir,val_data_dir)\n",
    "model = build_vgg19_based_model().to(device)\n",
    "loss_func = nn.CrossEntropyLoss(weight = class_weights, reduction = 'mean').to(device) # 가중 손실 함수\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', patience = 5, factor = 0.1,\n",
    "                                                       min_lr = 1e-12,verbose = True)  # lr scheduling\n",
    "early_stopping = EarlyStopping(patience = 10, verbose = False) # 조기 종료(사용자 정의 모듈)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "684838e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = './DataSet/archive/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c48f3bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 테스트 이미지 경로 불러오기\n",
    "\n",
    "test_angry_imgs = list_image_file(test_data_dir,'angry')\n",
    "test_disgust_imgs = list_image_file(test_data_dir,'disgust')\n",
    "test_fear_imgs = list_image_file(test_data_dir,'fear')\n",
    "test_happy_imgs = list_image_file(test_data_dir,'happy')\n",
    "test_neutral_imgs = list_image_file(test_data_dir,'neutral')\n",
    "test_sad_imgs = list_image_file(test_data_dir,'sad')\n",
    "test_surprise_imgs = list_image_file(test_data_dir,'surprise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cb84d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 테스트 이미지 전처리\n",
    "\n",
    "def preprocess_image(image):\n",
    "    transformer = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    tensor_image = transformer(image) # image: (C,H,W)\n",
    "    tensor_image = tensor_image.unsqueeze(0) # (B(batch),C,H,W)\n",
    "    \n",
    "    return tensor_image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1c141ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 예측을 위한 함수\n",
    "\n",
    "def model_predict(image,model):\n",
    "    tensor_image = preprocess_image(image) # 이미지(feature data)\n",
    "    prediction = model(tensor_image) # 예측\n",
    "    \n",
    "    _, pred_label1 = torch.max(prediction.detach(),dim = 1) # dim = 1 : 1차원으로 이미지를 가져오겠다.\n",
    "    print('pred_label1: ',pred_label1)\n",
    "    \n",
    "    pred_label = pred_label1.squeeze(0) # 차원 증가\n",
    "    print('pred_label2: ',pred_label)\n",
    "    \n",
    "    return pred_label.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24d96c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=256, out_features=7, bias=True)\n",
       "    (5): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 학습된 모델 불러오기\n",
    "\n",
    "ckpt = torch.load('./model_saved/model_adam_ver7.pth')\n",
    "\n",
    "model = build_vgg19_based_model()\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e331fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 이미지 파일을 RGB 3차원 배열로 가져오는 함수\n",
    "\n",
    "def get_RGB_image(data_dir,file_name):\n",
    "    image_file = os.path.join(data_dir,file_name) # 이미지 경로 설정\n",
    "    image = cv2.imread(image_file) # 이미지 열기\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB) # BGR -> RGB\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "424e74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 개수가 적은 감정에 개수 맞추기\n",
    "\n",
    "min_num_files = min(len(test_angry_imgs), len(test_disgust_imgs), len(test_fear_imgs),len(test_happy_imgs),\n",
    "                    len(test_neutral_imgs),len(test_sad_imgs),len(test_surprise_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a9dd58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae014461096f4e89b6135cebb6a42b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=109), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 예측 결과 시각화\n",
    "\n",
    "@interact(index = (0, min_num_files - 1))\n",
    "def show_result(index = 0):\n",
    "    # 테스트 이미지 파일 가져오기\n",
    "    angry_image = get_RGB_image(test_data_dir, test_angry_imgs[index])\n",
    "    disgust_image = get_RGB_image(test_data_dir, test_disgust_imgs[index])\n",
    "    fear_image = get_RGB_image(test_data_dir, test_fear_imgs[index])\n",
    "    happy_image = get_RGB_image(test_data_dir, test_happy_imgs[index])\n",
    "    neutral_image = get_RGB_image(test_data_dir, test_neutral_imgs[index])\n",
    "    sad_image = get_RGB_image(test_data_dir, test_sad_imgs[index])\n",
    "    surprise_image = get_RGB_image(test_data_dir, test_surprise_imgs[index])\n",
    "    \n",
    "    # 예측\n",
    "    prediction_1 = model_predict(angry_image, model)\n",
    "    prediction_2 = model_predict(disgust_image, model)\n",
    "    prediction_3 = model_predict(fear_image, model)\n",
    "    prediction_4 = model_predict(happy_image, model)\n",
    "    prediction_5 = model_predict(neutral_image, model)\n",
    "    prediction_6 = model_predict(sad_image, model)\n",
    "    prediction_7 = model_predict(surprise_image, model)\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(25, 17))\n",
    "    plt.subplot(241)\n",
    "    plt.title(f'Pred: {feelings_list[prediction_1]} | GT: Angry',fontsize = 35)\n",
    "    plt.imshow(angry_image)\n",
    "    plt.subplot(242)\n",
    "    plt.title(f'Pred: {feelings_list[prediction_2]} | GT: Disgust',fontsize = 35)\n",
    "    plt.imshow(disgust_image)\n",
    "    plt.subplot(243)\n",
    "    plt.title(f'Pred: {feelings_list[prediction_3]} | GT: Fear',fontsize = 35)\n",
    "    plt.imshow(fear_image)\n",
    "    plt.subplot(244)\n",
    "    plt.title(f'Pred: {feelings_list[prediction_4]} | GT: Happy',fontsize = 35)\n",
    "    plt.imshow(happy_image)\n",
    "    plt.subplot(245)\n",
    "    plt.title(f'Pred: {feelings_list[prediction_5]} | GT: Neutral',fontsize = 35)\n",
    "    plt.imshow(neutral_image)\n",
    "    plt.subplot(246)\n",
    "    plt.title(f'Pred: {feelings_list[prediction_6]} | GT: Sad',fontsize = 35)\n",
    "    plt.imshow(sad_image)\n",
    "    plt.subplot(247)\n",
    "    plt.title(f'Pred: {feelings_list[prediction_7]} | GT: Surprise',fontsize = 35)\n",
    "    plt.imshow(surprise_image)\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
