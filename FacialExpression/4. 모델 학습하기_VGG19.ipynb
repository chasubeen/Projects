{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293b64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab526671",
   "metadata": {},
   "source": [
    "# **1. 모델 검증을 위한 accuracy 생성하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49e78616",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() \n",
    "def get_accuracy(image,target,model):\n",
    "    batch_size = image.shape[0] \n",
    "    prediction = model(image) # 예측 \n",
    "    _,pred_label = torch.max(prediction,dim = 1) # 예측이 어느 클래스에 속하는지 확률이 가장 높은 1개 선택\n",
    "    is_correct = (pred_label == target)\n",
    "    \n",
    "    return is_correct.cpu().numpy().sum() / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2866b3",
   "metadata": {},
   "source": [
    "- Project(2)의 Dice coefficient 방법도 고려해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e9c891",
   "metadata": {},
   "source": [
    "# **2. 학습을 위한 함수 구현하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "122cabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb4d59",
   "metadata": {},
   "source": [
    "- AutoML 등을 이용해서 hyper-parameter tuning을 진행하려면 대충 이쯤에서 auto를 적용하면 될 것 같은데?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6928c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloaders,model,optimizer,loss_func,device):\n",
    "    losses = {} # loss값 저장\n",
    "    accuracies = {} # 정확도 값 저장\n",
    "    \n",
    "    for tv in ['train','valid']:\n",
    "        ### loss, accuracy를 계속 갱신\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        \n",
    "        if tv == 'train': # 학습\n",
    "            model.train()\n",
    "        else: # 검증\n",
    "            model.eval()\n",
    "        \n",
    "        for index,batch in enumerate(dataloaders[tv]):\n",
    "            image = batch['image'].to(device) # feature data(이미지)\n",
    "            target = batch['target'].squeeze(dim = 1).to(device) # label data(감정), 1차원으로 차원 축소 진행\n",
    "            \n",
    "            ### 역전파 적용\n",
    "            with torch.set_grad_enabled(tv == 'train'): \n",
    "                prediction = model(image) # label 예측\n",
    "                loss = loss_func(prediction,target) # loss값 계산\n",
    "                \n",
    "                if tv == 'train':\n",
    "                    optimizer.zero_grad() # 한 번의 학습 완료 -> gradient를 0으로 초기화\n",
    "                    loss.backward() # 역전파\n",
    "                    optimizer.step() # 가중치 업데이트\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "            running_correct += get_accuracy(image,target,model)\n",
    "            \n",
    "            if tv == 'train':\n",
    "                if index % 100 == 0:\n",
    "                    print(f\"{index}/{len(dataloaders['train'])} - Running loss: {loss.item()}\")\n",
    "        losses[tv] = running_loss / len(dataloaders[tv])\n",
    "        accuracies[tv] = running_correct / len(dataloaders[tv])\n",
    "    return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e04ecde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습이 잘 된 모델 저장\n",
    "\n",
    "def save_best_model(model_state,model_name,save_dir = './best_model'):\n",
    "    os.makedirs(save_dir,exist_ok = True) # 경로 존재 시 덮어쓰기, 없는 경우 새로 생성\n",
    "    torch.save(model_state,os.path.join(save_dir,model_name)) # 모델 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a71734",
   "metadata": {},
   "source": [
    "# **3. 모델 학습 수행**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1986bb81",
   "metadata": {},
   "source": [
    "- 이전에 구현한 코드들을 모두 합치기(for 코드 동작)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "806adbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing Libraries\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc18c93",
   "metadata": {},
   "source": [
    "### **DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "266dab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = './DataSet/archive/train/'\n",
    "train_data_df = pd.read_csv(os.path.join(train_data_dir,'train.csv'))\n",
    "\n",
    "# 분류에 사용할 class 정의(7개의 감정들)\n",
    "feelings_list = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "IMAGE_SIZE = 48\n",
    "\n",
    "\n",
    "### 이미지 파일 경로를 리스트 형태로 저장하기 위한 함수\n",
    "def list_image_file(data_dir,sub_dir):\n",
    "    image_files = []\n",
    "    \n",
    "    images_dir = os.path.join(data_dir,sub_dir)\n",
    "    for file_path in os.listdir(images_dir):\n",
    "        image_files.append(os.path.join(sub_dir,file_path))\n",
    "    return image_files\n",
    "\n",
    "class Feeling_dataset(Dataset):\n",
    "    ### 생성자\n",
    "    def __init__(self,data_dir,transform = None):\n",
    "        self.data_dir = data_dir # 데이터가 저장된 상위 directory\n",
    "        \n",
    "        angry_imgs = list_image_file(data_dir,'angry')\n",
    "        disgust_imgs = list_image_file(data_dir,'disgust')\n",
    "        fear_imgs = list_image_file(data_dir,'fear')\n",
    "        happy_imgs = list_image_file(data_dir,'happy')\n",
    "        neutral_imgs = list_image_file(data_dir,'neutral')\n",
    "        sad_imgs = list_image_file(data_dir,'sad')\n",
    "        surprise_imgs = list_image_file(data_dir,'surprise')\n",
    "        \n",
    "        # 모든 사진들의 경로를 하나의 리스트에 저장\n",
    "        self.files_path = angry_imgs + disgust_imgs + fear_imgs + happy_imgs + neutral_imgs + sad_imgs + surprise_imgs\n",
    "        self.transform = transform\n",
    "    \n",
    "    ### 데이터 개수\n",
    "    def __len__(self):\n",
    "        return len(self.files_path) # 전체 데이터 개수\n",
    "    \n",
    "    ### getitem\n",
    "    def __getitem__(self,index):\n",
    "        # image(feature data)\n",
    "        image_file = os.path.join(self.data_dir,self.files_path[index])\n",
    "        image = cv2.imread(image_file)\n",
    "        image = cv2.resize(image,dsize = (IMAGE_SIZE,IMAGE_SIZE),interpolation = cv2.INTER_LINEAR)\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # feeling(target data)\n",
    "        target = feelings_list.index(self.files_path[index].split(os.sep)[0])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            target = torch.Tensor([target]).long()\n",
    "            \n",
    "        return {'image':image,'target':target}\n",
    "\n",
    "    \n",
    "### 텐서 변환\n",
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # 정규화\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
    "                         std = [0.229, 0.224, 0.225]) \n",
    "    # ImageNet이 학습한 수백만장의 이미지의 RGB 각각의 채널 값을 주로 default로 활용\n",
    "])\n",
    "\n",
    "train_dset = Feeling_dataset(train_data_dir,transformer)\n",
    "\n",
    "\n",
    "### 데이터로더 구현\n",
    "def build_dataloader(train_data_dir,val_data_dir):\n",
    "    dataloaders = {}\n",
    "    train_dset = Feeling_dataset(train_data_dir,transformer)\n",
    "    dataloaders['train'] = DataLoader(train_dset,batch_size = 64,shuffle = True,drop_last = True)\n",
    "    \n",
    "    val_dset = Feeling_dataset(val_data_dir,transformer)\n",
    "    dataloaders['val'] = DataLoader(val_dset,batch_size = 16,shuffle = False,drop_last = False)\n",
    "    \n",
    "    return dataloaders\n",
    "\n",
    "train_data_dir = './DataSet/archive/train/'\n",
    "val_data_dir = train_data_dir = './DataSet/archive/valid/'\n",
    "dataloaders = build_dataloader(train_data_dir,val_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd26bd",
   "metadata": {},
   "source": [
    "### **Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2c179c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\users\\bin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "### 모델 생성 함수\n",
    "# 기존의 VGG19 모델 호출 -> head 부분 수정\n",
    "def build_vgg19_based_model(device_name = 'gpu'):\n",
    "    device = torch.device(device_name)\n",
    "    model = models.vgg19(pretrained = True) # 이미 학습된 vgg19 모델 불러오기\n",
    "    # 일반 NN Layer(FC Layer)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(output_size = (1,1)) # 각 구역의 평균값 출력\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Flatten(), # 평탄화\n",
    "        nn.Linear(512,256), # 512 -> 256\n",
    "        nn.ReLU(), # 활성화 함수\n",
    "        nn.Dropout(0.1), # 과적합 방지\n",
    "        nn.Linear(256,7), # 256 -> 7(7개의 감정으로 분류되니)\n",
    "        nn.Softmax() # 활성화 함수(각 클래스에 속할 확률 추정)\n",
    "    )\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "model = build_vgg19_based_model(device_name = 'gpu') # 모델 객체 생성\n",
    "\n",
    "\n",
    "### 손실 함수\n",
    "loss_func = nn.CrossEntropyLoss(reduction = 'mean')\n",
    "\n",
    "\n",
    "### Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = 1e-3,momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba769c1",
   "metadata": {},
   "source": [
    "### **Estimation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfe9d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델 검증을 위한 accuracy\n",
    "\n",
    "@torch.no_grad() \n",
    "def get_accuracy(image,target,model):\n",
    "    batch_size = image.shape[0] \n",
    "    prediction = model(image) # 예측 \n",
    "    _,pred_label = torch.max(prediction,dim = 1) # 예측이 어느 클래스에 속하는지 확률이 가장 높은 1개 선택\n",
    "    is_correct = (pred_label == target)\n",
    "    \n",
    "    return is_correct.cpu().numpy().sum() / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91115c5c",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f82ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "### Epoch을 1번 수행\n",
    "def train_one_epoch(dataloaders,model,optimizer,loss_func,device):\n",
    "    losses = {} # loss값 저장\n",
    "    accuracies = {} # 정확도 값 저장\n",
    "    \n",
    "    for tv in ['train','val']:\n",
    "        ### loss, accuracy를 계속 갱신\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        \n",
    "        if tv == 'train': # 학습\n",
    "            model.train()\n",
    "        else: # 검증\n",
    "            model.eval()\n",
    "        \n",
    "        for index,batch in enumerate(dataloaders[tv]):\n",
    "            image = batch['image'].to(device) # feature data(이미지)\n",
    "            target = batch['target'].squeeze(dim = 1).to(device) # label data(감정), 1차원으로 차원 축소 진행\n",
    "            \n",
    "            ### 역전파 적용\n",
    "            with torch.set_grad_enabled(tv == 'train'): \n",
    "                prediction = model(image) # label 예측\n",
    "                loss = loss_func(prediction,target) # loss값 계산\n",
    "                \n",
    "                if tv == 'train':\n",
    "                    optimizer.zero_grad() # 한 번의 학습 완료 -> gradient를 0으로 초기화\n",
    "                    loss.backward() # 역전파\n",
    "                    optimizer.step() # 가중치 업데이트\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "            running_correct += get_accuracy(image,target,model)\n",
    "            \n",
    "            if tv == 'train':\n",
    "                if index % 50 == 0:\n",
    "                    print(f\"{index}/{len(dataloaders['train'])} - Running loss: {loss.item()}\")\n",
    "        losses[tv] = running_loss / len(dataloaders[tv])\n",
    "        accuracies[tv] = running_correct / len(dataloaders[tv])\n",
    "    return losses, accuracies\n",
    "\n",
    "\n",
    "### 학습이 잘 된 모델 저장\n",
    "def save_best_model(model_state,model_name,save_dir = './best_model'):\n",
    "    os.makedirs(save_dir,exist_ok = True) # 경로 존재 시 덮어쓰기, 없는 경우 새로 생성\n",
    "    torch.save(model_state,os.path.join(save_dir,model_name)) # 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b7323e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### 경로 설정\n",
    "train_data_dir = './DataSet/archive/train/'\n",
    "val_data_dir = './DataSet/archive/valid/'\n",
    "\n",
    "\n",
    "### 필요한 요소들 준비\n",
    "dataloaders = build_dataloader(train_data_dir,val_data_dir)\n",
    "model = build_vgg19_based_model(device_name = 'gpu')\n",
    "loss_func = nn.CrossEntropyLoss(reduction = 'mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = 1E-3,momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38f5b38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/455 - Running loss: 1.9550526142120361\n",
      "50/455 - Running loss: 1.9691872596740723\n",
      "100/455 - Running loss: 1.8561758995056152\n",
      "150/455 - Running loss: 1.7850385904312134\n",
      "200/455 - Running loss: 1.8075505495071411\n",
      "250/455 - Running loss: 1.8191804885864258\n",
      "300/455 - Running loss: 1.7326956987380981\n",
      "350/455 - Running loss: 1.6888909339904785\n",
      "400/455 - Running loss: 1.6124577522277832\n",
      "450/455 - Running loss: 1.7323325872421265\n",
      "1/10-Train Loss:1.7769679190038326, Val Loss:1.687256036620391\n",
      "1/10-Train Acc:0.3832074175824176, Val Acc:0.47004585326953746\n",
      "0/455 - Running loss: 1.7564808130264282\n",
      "50/455 - Running loss: 1.6349341869354248\n",
      "100/455 - Running loss: 1.6700773239135742\n",
      "150/455 - Running loss: 1.7181757688522339\n",
      "200/455 - Running loss: 1.6958435773849487\n",
      "250/455 - Running loss: 1.6471099853515625\n",
      "300/455 - Running loss: 1.656583309173584\n",
      "350/455 - Running loss: 1.7747159004211426\n",
      "400/455 - Running loss: 1.6378729343414307\n",
      "450/455 - Running loss: 1.6540002822875977\n",
      "2/10-Train Loss:1.6674273186987574, Val Loss:1.6602982115327267\n",
      "2/10-Train Acc:0.5016483516483516, Val Acc:0.49416866028708134\n",
      "0/455 - Running loss: 1.5995997190475464\n",
      "50/455 - Running loss: 1.614485502243042\n",
      "100/455 - Running loss: 1.5027408599853516\n",
      "150/455 - Running loss: 1.5186161994934082\n",
      "200/455 - Running loss: 1.6085013151168823\n",
      "250/455 - Running loss: 1.6904141902923584\n",
      "300/455 - Running loss: 1.6888574361801147\n",
      "350/455 - Running loss: 1.653969645500183\n",
      "400/455 - Running loss: 1.6344764232635498\n",
      "450/455 - Running loss: 1.648965835571289\n",
      "3/10-Train Loss:1.6330605960154272, Val Loss:1.638117424228735\n",
      "3/10-Train Acc:0.5386675824175824, Val Acc:0.5164972089314195\n",
      "0/455 - Running loss: 1.5873943567276\n",
      "50/455 - Running loss: 1.527669072151184\n",
      "100/455 - Running loss: 1.562829852104187\n",
      "150/455 - Running loss: 1.6325953006744385\n",
      "200/455 - Running loss: 1.4816780090332031\n",
      "250/455 - Running loss: 1.5656710863113403\n",
      "300/455 - Running loss: 1.6338202953338623\n",
      "350/455 - Running loss: 1.6188035011291504\n",
      "400/455 - Running loss: 1.659071683883667\n",
      "450/455 - Running loss: 1.590436577796936\n",
      "4/10-Train Loss:1.6045582139885033, Val Loss:1.628091079101228\n",
      "4/10-Train Acc:0.5692994505494505, Val Acc:0.5270633971291866\n",
      "0/455 - Running loss: 1.5002706050872803\n",
      "50/455 - Running loss: 1.5344349145889282\n",
      "100/455 - Running loss: 1.596299409866333\n",
      "150/455 - Running loss: 1.5055617094039917\n",
      "200/455 - Running loss: 1.6694889068603516\n",
      "250/455 - Running loss: 1.5093110799789429\n",
      "300/455 - Running loss: 1.628666877746582\n",
      "350/455 - Running loss: 1.6329888105392456\n",
      "400/455 - Running loss: 1.650403380393982\n",
      "450/455 - Running loss: 1.5715579986572266\n",
      "5/10-Train Loss:1.5822944229775733, Val Loss:1.6000542112609797\n",
      "5/10-Train Acc:0.5940934065934066, Val Acc:0.560506379585327\n",
      "0/455 - Running loss: 1.5983085632324219\n",
      "50/455 - Running loss: 1.5535507202148438\n",
      "100/455 - Running loss: 1.5925577878952026\n",
      "150/455 - Running loss: 1.5073738098144531\n",
      "200/455 - Running loss: 1.5374155044555664\n",
      "250/455 - Running loss: 1.608793020248413\n",
      "300/455 - Running loss: 1.5227634906768799\n",
      "350/455 - Running loss: 1.6512928009033203\n",
      "400/455 - Running loss: 1.4852732419967651\n",
      "450/455 - Running loss: 1.4907100200653076\n",
      "6/10-Train Loss:1.563598464347504, Val Loss:1.5892488302891714\n",
      "6/10-Train Acc:0.6134271978021978, Val Acc:0.5717454146730462\n",
      "0/455 - Running loss: 1.5415617227554321\n",
      "50/455 - Running loss: 1.5381022691726685\n",
      "100/455 - Running loss: 1.546830415725708\n",
      "150/455 - Running loss: 1.4691540002822876\n",
      "200/455 - Running loss: 1.6176350116729736\n",
      "250/455 - Running loss: 1.654805064201355\n",
      "300/455 - Running loss: 1.6511485576629639\n",
      "350/455 - Running loss: 1.6004815101623535\n",
      "400/455 - Running loss: 1.6120803356170654\n",
      "450/455 - Running loss: 1.5848886966705322\n",
      "7/10-Train Loss:1.5460679641136756, Val Loss:1.5854865019781548\n",
      "7/10-Train Acc:0.6328296703296703, Val Acc:0.5753090111642742\n",
      "0/455 - Running loss: 1.5416674613952637\n",
      "50/455 - Running loss: 1.5861339569091797\n",
      "100/455 - Running loss: 1.4447195529937744\n",
      "150/455 - Running loss: 1.5760295391082764\n",
      "200/455 - Running loss: 1.573939561843872\n",
      "250/455 - Running loss: 1.5603082180023193\n",
      "300/455 - Running loss: 1.514297366142273\n",
      "350/455 - Running loss: 1.4964008331298828\n",
      "400/455 - Running loss: 1.5316972732543945\n",
      "450/455 - Running loss: 1.4755206108093262\n",
      "8/10-Train Loss:1.5311921331908676, Val Loss:1.595615256773798\n",
      "8/10-Train Acc:0.6470810439560439, Val Acc:0.5635217304625199\n",
      "0/455 - Running loss: 1.5164144039154053\n",
      "50/455 - Running loss: 1.606601595878601\n",
      "100/455 - Running loss: 1.4847919940948486\n",
      "150/455 - Running loss: 1.5435056686401367\n",
      "200/455 - Running loss: 1.4435616731643677\n",
      "250/455 - Running loss: 1.4768646955490112\n",
      "300/455 - Running loss: 1.5507644414901733\n",
      "350/455 - Running loss: 1.6007006168365479\n",
      "400/455 - Running loss: 1.4977881908416748\n",
      "450/455 - Running loss: 1.4621031284332275\n",
      "9/10-Train Loss:1.5199993225244375, Val Loss:1.5745221716270112\n",
      "9/10-Train Acc:0.6587225274725275, Val Acc:0.5851774322169059\n",
      "0/455 - Running loss: 1.4758520126342773\n",
      "50/455 - Running loss: 1.5493590831756592\n",
      "100/455 - Running loss: 1.4544559717178345\n",
      "150/455 - Running loss: 1.53116774559021\n",
      "200/455 - Running loss: 1.44140625\n",
      "250/455 - Running loss: 1.4986577033996582\n",
      "300/455 - Running loss: 1.5066554546356201\n",
      "350/455 - Running loss: 1.4751018285751343\n",
      "400/455 - Running loss: 1.5141501426696777\n",
      "450/455 - Running loss: 1.5371990203857422\n",
      "10/10-Train Loss:1.5074405109489357, Val Loss:1.5649008123498214\n",
      "10/10-Train Acc:0.6741414835164835, Val Acc:0.5959928229665071\n",
      "Bset Accuracy: 0.5959928229665071\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "best_acc = 0.0\n",
    "train_loss,train_accuracy = [],[]\n",
    "val_loss,val_accuracy = [],[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses, accuracies = train_one_epoch(dataloaders, model, optimizer, loss_func, device)\n",
    "    train_loss.append(losses['train'])\n",
    "    val_loss.append(losses['val'])\n",
    "    train_accuracy.append(accuracies['train'])\n",
    "    val_accuracy.append(accuracies['val'])\n",
    "    \n",
    "    print(f\"{epoch+1}/{num_epochs}-Train Loss:{losses['train']}, Val Loss:{losses['val']}\")\n",
    "    print(f\"{epoch+1}/{num_epochs}-Train Acc:{accuracies['train']}, Val Acc:{accuracies['val']}\")\n",
    "    \n",
    "    # deepcopy: 배열의 내부 객체까지 복사를 해서 사용\n",
    "    # copy: 배열의 내부 객체까지 깊은 복사를 해주지 않음\n",
    "    if (epoch > 3) and (accuracies['val'] > best_acc):\n",
    "        best_acc = accuracies['val']\n",
    "        best_model = copy.deepcopy(model.state_dict()) \n",
    "        save_best_model(best_model, f'model_{epoch+1:02d}.pth')\n",
    "\n",
    "print(f'Bset Accuracy: {best_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ac5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss, Accuracy 시각화\n",
    "\n",
    "plt.figure(figsize = (6,5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(211)\n",
    "plt.plot(train_loss,label = \"train\")\n",
    "plt.plot(val_loss,label = \"val\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.grid(\"on\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(212)\n",
    "plt.plot(train_accuracy,label = \"train\")\n",
    "plt.plot(val_accuracy,label = \"val\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.grid(\"on\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d30208",
   "metadata": {},
   "source": [
    "- 성능 최적화가 필요해 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa2e2f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
