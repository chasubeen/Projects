# **[포스팅 리뷰 - 이미지 데이터의 클래스 불균형을 개선하는 4가지 방법]**
https://towardsdatascience.com/4-ways-to-improve-class-imbalance-for-image-data-9adec8f390f1

# **1. Image Chipping**
- 원본 이미지가 NN input layer의 input size보다 큰 경우 활용할 수 있는 방법
- Down Sampling의 대안으로 원본 이미지를 더 작은 타일로 조각내는 것
  - 정보 소실을 막기 위해 약간의 overlapping을 적용해도 ok
- **원본 이미지 down sampling**
  - 주요 정보의 소실 위험
  - 작은 개체를 가지는 클래스에 대한 정밀도가 낮아짐
  - 소수 클래스의 크기 <<<< 다수 클래스의 크기
    - 다운 샘플링은 소수 클래스에 불균형적으로 영향을 미침
    
- 해상도가 보존됨 
- 성능을 향상시킬 수 있지만 runtime 증가율도 고려해야 함

# **2. 거의 동일한 클래스들을 병합**
- 각 클래스의 데이터 포인트가 많지 않은 경우 낮은 픽셀 항목에 대한 클래스 세분화 수준은 모델이 학습하기 어려울 수 있음
- 비슷한 속성을 가진 클래스들을 단일 병합 클래스로 병합
  - 클래스 수를 줄이고 클래스 분포 불균형을 약간 줄임
- 가능성은 있지만 실행 가능성은 도메인 및 최종 사용자 워크플로우에 따라 달라짐 -> 비추

# **3. 특정 클래스 resampling**
- 훈련 세트에서 클래스 표현을 조정

### **Over Sampling**
- 소수 클래스의 엔트리를 다수 클래스의 수량에 맞게 증가
  - 합성 데이터를 생성하거나 소수 클래스의 항목을 본질적으로 복사하는 등 여러 가지 방법으로 수행될 수 있음
  - ex> sklearn의 'resample' 또는 TensorFlow의 tf.datasampler 또는 PyTorch의 WeightedRandomSampler)
- 단점: 과도하게 샘플링 된 클래스의 과적합을 초래할 수 있음

### **Down Sampling**
- 소수 클래스 내의 수량과 일치하도록 다수 클래스에서 항목을 제거하는 것
- 단점: 
  - 데이터 포인트를 제거하면 중요한 정보가 제거되거나 실제 데이터에 대한 일반화가 제대로 이루어지지 않을 수 있음
  - 또는 불균형이 너무 심하여 언더 샘플링의 결과 데이터 세트의 크기가 너무 작을 수 있음

<img src = "https://user-images.githubusercontent.com/98953721/203377941-ce3121e5-c41c-4a17-afd8-ca2907958dc6.png" width = 500 height = 300>

※ validation 세트의 데이터가 train 세트에 포함되지 않도록 하기 위해서 데이터를 train 및 valid 세트로 분할한 후 train 세트에 대해서만 resampling이 수행되도록 해야 함


# **4. 손실 함수 조정**
- 각각의 mistake를 동등하게 다루는 대신, 소수 클래스에서의 mistake를 보통의 클래스에서의 실수보다 더 중요하게 취급
- 손실 함수는 각 클래스의 예측 확률에 기반하여 동적으로 적용
- focusing parameter: **γ** ---> loss 조정
- 총 손실의 관점에서 과도하게 표현된 클래스의 지배력을 감소
- 병합 클래스 데이터 세트의 경우 focal loss 구현 시 소수 클래스의 평균 정밀도가 증가하고 다수 클래스의 평균 정밀도가 상대적으로 양호하게 보존됨


# **[블로그 리뷰 - 클래스 불균형 다루기(가중 무작위 샘플링, 가중 손실 함수)]**
###### https://yeong-jin-data-blog.tistory.com/entry/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98-%EC%8A%A4%ED%84%B0%EB%94%94-%ED%81%B4%EB%9E%98%EC%8A%A4-%EB%B6%88%EA%B7%A0%ED%98%95-%EB%8B%A4%EB%A3%A8%EA%B8%B0-%EA%B0%80%EC%A4%91-%EB%AC%B4%EC%9E%91%EC%9C%84-%EC%83%98%ED%94%8C%EB%A7%81-%EA%B0%80%EC%A4%91-%EC%86%90%EC%8B%A4-%ED%95%A8%EC%88%98
# **1. 가중 무작위 샘플링(Weighted Random Sampling)**
- 딥러닝 모델 학습 시 전체 데이터기 불균형 하더라도, mini batch를 뽑을 때 각 클래스를 균형있게 뽑아서 학습시킬 수 있도록 하는 기법
- 데이터를 불러와 가중치를 생성하고 Tensor로 변환
  - 모든 데이터에 대한 각각의 가중치가 생기고, 각 클래스의 가중치 합은 1이다.
- 데이터로더 생성 시 가중치를 넘겨준다.

```Python
sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) # weights: 가중치 행렬(Tensor 형)
```

# **2. 가중 손실 함수**
- 수가 적은 클래스에 대해서 더 큰 가중치를 부여해서 업데이트 균형을 맞추는 것
- Pytorch의 **nn.CrossEntropyLoss**는 가중 손실 함수를 제공
  - 미리 정의된 파라미터 weight 값을 부여하면 된다.

```Python
class_weights = torch.FloatTensor(weights).to(device) # 가중 무작위 샘플링 시 구한 가중치를 가져와 Float Tensor로 변환

loss_fn = nn.CrossEntropyLoss(weight = class_weights) # 손실 함수에 가중치 적용
```







