# **[이미지 데이터의 클래스 불균형을 개선하는 4가지 방법]**
https://towardsdatascience.com/4-ways-to-improve-class-imbalance-for-image-data-9adec8f390f1

# **1. Image Chipping**
- 원본 이미지가 NN input layer의 input size보다 큰 경우 활용할 수 있는 방법
- Down Sampling의 대안으로 원본 이미지를 더 작은 타일로 조각내는 것
  - 정보 소실을 막기 위해 약간의 overlapping을 적용해도 ok
- **원본 이미지 down sampling**
  - 주요 정보의 소실 위험
  - 작은 개체를 가지는 클래스에 대한 정밀도가 낮아짐
  - 소수 클래스의 크기 <<<< 다수 클래스의 크기
    - 다운 샘플링은 소수 클래스에 불균형적으로 영향을 미침
    
- 해상도가 보존됨 
- 성능을 향상시킬 수 있지만 runtime 증가율도 고려해야 함

# **2. 거의 동일한 클래스들을 병합**
- 각 클래스의 데이터 포인트가 많지 않은 경우 낮은 픽셀 항목에 대한 클래스 세분화 수준은 모델이 학습하기 어려울 수 있음
- 비슷한 속성을 가진 클래스들을 단일 병합 클래스로 병합
  - 클래스 수를 줄이고 클래스 분포 불균형을 약간 줄임
- 가능성은 있지만 실행 가능성은 도메인 및 최종 사용자 워크플로우에 따라 달라짐 -> 비추

# **3. 특정 클래스 resampling**
- 훈련 세트에서 클래스 표현을 조정

### **Over Sampling**
- 소수 클래스의 엔트리를 다수 클래스의 수량에 맞게 증가
  - 합성 데이터를 생성하거나 소수 클래스의 항목을 본질적으로 복사하는 등 여러 가지 방법으로 수행될 수 있음
  - ex> sklearn의 'resample' 또는 TensorFlow의 tf.datasampler 또는 PyTorch의 WeightedRandomSampler)
- 단점: 과도하게 샘플링 된 클래스의 과적합을 초래할 수 있음

### **Down Sampling**
- 소수 클래스 내의 수량과 일치하도록 다수 클래스에서 항목을 제거하는 것
- 단점: 
  - 데이터 포인트를 제거하면 중요한 정보가 제거되거나 실제 데이터에 대한 일반화가 제대로 이루어지지 않을 수 있음
  - 또는 불균형이 너무 심하여 언더 샘플링의 결과 데이터 세트의 크기가 너무 작을 수 있음

<img src = "https://user-images.githubusercontent.com/98953721/203377941-ce3121e5-c41c-4a17-afd8-ca2907958dc6.png" width = 500 height = 300>

※ validation 세트의 데이터가 train 세트에 포함되지 않도록 하기 위해서 데이터를 train 및 valid 세트로 분할한 후 train 세트에 대해서만 resampling이 수행되도록 해야 함


# **4. 손실 함수 조정**
- 각각의 mistake를 동등하게 다루는 대신, 소수 클래스에서의 mistake를 보통의 클래스에서의 실수보다 더 중요하게 취급
- 손실 함수는 각 클래스의 예측 확률에 기반하여 동적으로 적용
- focusing parameter: **γ** ---> loss 조정
- 총 손실의 관점에서 과도하게 표현된 클래스의 지배력을 감소
- 병합 클래스 데이터 세트의 경우 focal loss 구현 시 소수 클래스의 평균 정밀도가 증가하고 다수 클래스의 평균 정밀도가 상대적으로 양호하게 보존됨










