# **0. Reference**
- [ê¸°ìˆ ë¸”ë¡œê·¸_ê³¼ì í•© ë°©ì§€ë¥¼ í†µí•œ ëª¨ë¸ ì„±ëŠ¥ ê°œì„ ](https://yeong-jin-data-blog.tistory.com/entry/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98-%EC%8A%A4%ED%84%B0%EB%94%94-%EA%B3%BC%EC%A0%81%ED%95%A9-%EB%B0%A9%EC%A7%80%EB%A5%BC-%ED%86%B5%ED%95%9C-%EB%AA%A8%EB%8D%B8-%EC%84%B1%EB%8A%A5-%EA%B0%9C%EC%84%A0-%EB%B0%A9%EB%B2%95)  
- [êµë€ ë¼ë²¨ë§ ê´€ë ¨ ë…¼ë¬¸](https://arxiv.org/abs/1605.00055)

# **1. ë°ì´í„° ì¦ì‹(Data Augmentation)**
- í•™ìŠµì— í•„ìš”í•œ ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ì´ ì–´ë ¤ìš´ ê²½ìš° ê¸°ì¡´ ë°ì´í„°ë¥¼ ì¦ì‹í•˜ëŠ” ë°©ë²•
- êµ¬ì²´ì ì¸ ë°©ë²•ìœ¼ë¡œëŠ” í†µê³„ì  ê¸°ë²•, ë‹¨ìˆœ ë³€í˜•, ìƒì„± ëª¨ë¸ ì‚¬ìš© ë“±ì´ ìˆìŒ

### **1-1.  torchvision.transforms**
- íŒŒì´í† ì¹˜ì—ì„œ ê³µì‹ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” ëª¨ë“ˆ
- ë¨¼ì € **DataLoader**ë¥¼ ì •ì˜í•œ í›„, DataLoader í´ë˜ìŠ¤ì˜ **__getitem__** ë©”ì†Œë“œì—ì„œ transform í˜¸ì¶œ
  - DataLoader í´ë˜ìŠ¤ëŠ” **__getitem__** ë©”ì†Œë“œë¥¼ í†µí•´ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¨ í›„ ë°ì´í„° augmentationì„ ì§„í–‰ 
- ì´í›„ transform íŒŒë¼ë¯¸í„°ì— ì €ì¥ë˜ì–´ ìˆëŠ” augmentation ê·œì¹™ì— ë”°ë¼ augmentationì´ ì§„í–‰ë¨
- ì½”ë“œ
  - [Reference](https://pseudo-lab.github.io/Tutorial-Book/chapters/object-detection/Ch3-preprocessing.html) 

**1) DataLoader ì •ì˜**  
```Python
from PIL import Image
import cv2
import numpy as np
import time
import torch
import torchvision
from torch.utils.data import Dataset
from torchvision import transforms
import albumentations
import albumentations.pytorch
from matplotlib import pyplot as plt
import os
import random

class TorchvisionMaskDataset(Dataset):
    def __init__(self, path, transform=None):
        self.path = path
        self.imgs = list(sorted(os.listdir(self.path)))
        self.transform = transform
    def __len__(self):
        return len(self.imgs)
    def __getitem__(self, idx):
        file_image = self.imgs[idx]
        file_label = self.imgs[idx][:-3] + 'xml'
        img_path = os.path.join(self.path, file_image)
        
        if 'test' in self.path:
            label_path = os.path.join("test_annotations/", file_label)
        else:
            label_path = os.path.join("annotations/", file_label)

        img = Image.open(img_path).convert("RGB")
        
        target = generate_target(label_path)
        
        if self.transform:
            img = self.transform(img)

        return img, target
```

**2) transform ì •ì˜**  
```Python  
torchvision_transform = transforms.Compose([
    transforms.Resize((300, 300)), 
    transforms.RandomCrop(224),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),
    transforms.RandomHorizontalFlip(p = 1),
    transforms.ToTensor(),
])
```
- Resize(W,H): ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
- RandomCrop(size): ì‚¬ì§„ì„ ì„ì˜ë¡œ size í¬ê¸°ë¡œ ìë¥´ê¸°
- ColorJitter(brightness, contrast, saturation, hue): ì´ë¯¸ì§€ì˜ ë°ê¸°(brightness), ëŒ€ë¹„(contrast), ì±„ë„(saturation), ìƒ‰ì¡°(hue) ë³€í˜•
- RandomHorizontalFlip(p): ì •ì˜í•œ pì˜ í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „

**3) í™œìš©**  
- ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ë•Œ transform ìˆ˜í–‰  
```Python
torchvision_dataset = TorchvisionMaskDataset(
    path = 'images/',
    transform = torchvision_transform
) 
```

- ê°ì²´ íƒì§€ìš© ëª¨ë¸ êµ¬ì¶•ì„ ìœ„í•œ ì´ë¯¸ì§€ augmentation ê¸°ëŠ¥ì€ albumentationsì—ì„œë§Œ ì œê³µí•œë‹¤ëŠ” ë‹¨ì ì´ ì¡´ì¬  
(ì‚¬ì‹¤ ìš°ë¦¬ í”„ë¡œì íŠ¸ì™€ëŠ” ìƒê´€ì´ ì—†ì–´ ê·¸ë˜ì„œ..^-^)


### **1-2. albumentations ëª¨ë“ˆ**
- OpenCVì™€ ê°™ì€ ì˜¤í”ˆ ì†ŒìŠ¤ ì»´í“¨í„° ë¹„ì ¼ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ìµœì í™”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
- ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë³´ë‹¤ ë” ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„ ë° ê¸°íƒ€ ê¸°ëŠ¥ì„ ì œê³µ


# **2. ì¡°ê¸° ì¢…ë£Œ(Early Stopping)**

# **3. L2 ì •ê·œí™”(L2 ê·œì œ)**
- ê·œì œ(Regularization): í•™ìŠµì´ ê³¼ëŒ€ì í•©ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³ ì ì¼ì¢…ì˜ penaltyë¥¼ ë¶€ì—¬í•˜ëŠ” ê²ƒ

### **3-1.L2 ì •ê·œí™”(L2 ê·œì œ)**
- ê° ê°€ì¤‘ì¹˜ ì œê³±ì˜ í•©ì— ê·œì œ ê°•ë„ë¥¼ ê³±í•œ ê°’($Error = MSE + Î±ğ‘¤^2$)
- ì›í˜•ì˜ ê²½ê³„ë¥¼ ë§Œë“¤ì–´ì„œ í•™ìŠµ ë°ì´í„°ì…‹ì˜ ìµœì  ì§€ì ì¸ w* ì— ë„ë‹¬í•˜ì§€ ëª»í•˜ê²Œ í•˜ê³  ê²½ê³„ ë‚´ë¶€ì˜ v* ê¹Œì§€ë§Œ ë„ë‹¬í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë°©ì‹
- Optimizerë¡œ Adamì„ ì‚¬ìš©í•  ê²½ìš° **weight_decay** íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŒ
  - ê°’ì´ í´ìˆ˜ë¡ ê·œì œ ê°•ë„ê°€ ê°•í•œ ê²ƒì„ ì˜ë¯¸ -> ê°€ì¤‘ì¹˜ê°€ ë” ë§ì´ ê°ì†Œë¨
  - ë¦¿ì§€(Ridge) ëª¨ë¸ì— ì ìš©ëœë‹¤.
  - ì½”ë“œ
  ```Python
  optimizer = optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-3)
  ```

### **3-2. L1 ì •ê·œí™”(L1 ê·œì œ)**
- ê°€ì¤‘ì¹˜ì˜ í•©ì„ ë”í•œ ê°’ì— ê·œì œ ê°•ë„ë¥¼ ê³±í•˜ì—¬ ì˜¤ì°¨ì— ë”í•œ ê°’($Error = MSE + Î±|w|$)
- ì–´ë–¤ ê°€ì¤‘ì¹˜ëŠ” ì‹¤ì œë¡œ 0ì´ ëœë‹¤. => ëª¨ë¸ì—ì„œ ì™„ì „íˆ ì œì™¸ë˜ëŠ” íŠ¹ì„±ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ
- ë¼ì˜(Lasso) ëª¨ë¸ì— ì ìš©ëœë‹¤.


# **4. ë“œë¡­ì•„ì›ƒ(Dropout)**
- í›ˆë ¨(train)í•  ë•Œ **ì¼ì • ë¹„ìœ¨**ì˜ ë‰´ëŸ°ë§Œ ì‚¬ìš©í•˜ê³ , ë‚˜ë¨¸ì§€ ë‰´ëŸ°ì— í•´ë‹¹í•˜ëŠ” ê°€ì¤‘ì¹˜ëŠ” ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠëŠ” ë°©ë²•
  - ë…¸ë“œë¥¼ ì„ì˜ë¡œ ë„ë©´ì„œ í•™ìŠµí•˜ëŠ” ë°©ë²•
  - ì€ë‹‰ì¸µ(hidden layer)ì— ë°°ì¹˜ëœ ë…¸ë“œ ì¤‘ ì¼ë¶€ë¥¼ ì„ì˜ë¡œ ë„ë©´ì„œ í•™ìŠµ
- í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ê³¼ì í•©(overfitting)ì„ ì–µì œí•  ìˆ˜ ìˆìŒ
- í›ˆë ¨ ì‹œê°„ì´ ê¸¸ì–´ì§€ëŠ” ë‹¨ì ì´ ì¡´ì¬
- ì¶œë ¥ì¸µì—ì„œëŠ” ì˜ˆì¸¡ê°’ì„ ì‚°ì¶œí•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ë“œë¡­ì•„ì›ƒì„ ì‚¬ìš©í•´ì„œëŠ” ì•ˆë˜ê³ , **ì€ë‹‰ì¸µ**ì— ëŒ€í•´ì„œë§Œ ì‚¬ìš©í•´ì•¼ í•¨
- ëª¨ë¸ í‰ê°€ ë‹¨ê³„ì—ì„œëŠ” ë“œë¡­ì•„ì›ƒì„ ì‹¤ì‹œí•˜ì§€ ì•Šì€ ëª¨ë¸ë¡œ í‰ê°€í•´ì•¼ í•¨
  - íŒŒì´í† ì¹˜ì—ì„œëŠ” ```.eval()``` ë¥¼ í†µí•´ì„œ ì›ë˜ ì „ì²´ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ

# **5. ë°°ì¹˜ ì •ê·œí™”(Batch Normalization)**
- ê¸°ìš¸ê¸° ì†Œë©¸(gradient vanishing)ì´ë‚˜ ê¸°ìš¸ê¸° í­ë°œ(gradient exploding)ê³¼ ê°™ì€ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•
- ì¼ë°˜ì ìœ¼ë¡œ ê¸°ìš¸ê¸° ì†Œë©¸ì´ë‚˜ í­ë°œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì†ì‹¤ í•¨ìˆ˜ë¡œ ```ReLu```ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì´ˆê¹ƒê°’ íŠœë‹, í•™ìŠµë¥ (learning rate) ë“±ì„ ì¡°ì •
- ë¶„ì‚°ëœ ë¶„í¬ë¥¼ ì •ê·œë¶„í¬ë¡œ ë§Œë“¤ê¸° ìœ„í•´ í‘œì¤€í™”ì™€ ìœ ì‚¬í•œ ë°©ì‹ì„ ë¯¸ë‹ˆ ë°°ì¹˜ì— ì ìš©í•˜ì—¬ **í‰ê·  = 0, í‘œì¤€í¸ì°¨ = 1**ë¡œ ìœ ì§€ë˜ë„ë¡ í•¨
- ë‹¨ê³„>
  1) ë¯¸ë‹ˆ ë°°ì¹˜ í‰ê·  êµ¬í•˜ê¸°
  2) ë¯¸ë‹ˆ ë°°ì¹˜ì˜ ë¶„ì‚°, í‘œì¤€í¸ì°¨ êµ¬í•˜ê¸°
  3) ì •ê·œí™” ìˆ˜í–‰
  4) ìŠ¤ì¼€ì¼(scale) ì¡°ì •(ë°ì´í„° ë¶„í¬ ì¡°ì •)
- ì¥ì : ë§¤ ë‹¨ê³„ë§ˆë‹¤ í™œì„±í™” í•¨ìˆ˜ë¥¼ ê±°ì¹˜ë©´ì„œ ë°ì´í„°ì…‹ ë¶„í¬ë¥¼ **ì¼ì •**í•˜ê²Œ ìœ ì§€ì‹œí‚¬ ìˆ˜ ìˆìŒ => ì†ë„ í–¥ìƒ
- ë‹¨ì 
  - ë°°ì¹˜ í¬ê¸°ê°€ ì‘ì„ ë•ŒëŠ” ì •ê·œí™” ê°’ì´ ê¸°ì¡´ ê°’ê³¼ ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ í›ˆë ¨ë  ìˆ˜ ìˆìŒ
    - ë¶„ì‚°ì´ 0ì¸ ê²½ìš° ì •ê·œí™” ìì²´ê°€ ìˆ˜í–‰ë˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ìƒê¸¸ ìˆ˜ ìˆìŒ
  - RNNì˜ ê²½ìš° ë„¤íŠ¸ì›Œí¬ ê³„ì¸µë³„ë¡œ ë¯¸ë‹ˆ ì •ê·œí™”ë¥¼ ì ìš©í•´ì•¼ í•¨ -> ëª¨ë¸ì´ ë” ë³µì¡í•´ì§€ë©´ì„œ ë¹„íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŒ
- ì½”ë“œ
```Python
torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)
```

# **6. êµë€ ë¼ë²¨ (Disturb Label) / êµë€ ê°’(Disturb Value)**
- ë¶„ë¥˜(classification) ë¬¸ì œì—ì„œ ì¼ì • ë¹„ìœ¨ì˜ ë¼ë²¨ì„ ì˜ë„ì ìœ¼ë¡œ **ì˜ëª»ëœ** ë¼ë²¨ë¡œ ë§Œë“¤ì–´ì„œ í•™ìŠµì„ ë°©í•´í•˜ëŠ” ë°©ì‹
  - ë‹¨ìˆœí•œ ë°©ì‹ì´ì§€ë§Œ ê³¼ì í•©ì„ íš¨ê³¼ì ìœ¼ë¡œ ë§‰ì„ ìˆ˜ ìˆìŒ
- ì†ì‹¤ì¸µ(loss layer)ì— ê·œì œë¥¼ ë‘ëŠ” ë°©ì‹
- ì½”ë“œ  
**1. DisturbLabel ê°ì²´ ì •ì˜**  
```Python
class DisturbLabel(torch.nn.Module):
    def __init__(self, alpha, num_classes): 
    #alpha: êµë€ ë¼ë²¨ë¡œ ì²˜ë¦¬í•  ë¹„ìœ¨, num_classes: ë°ì´í„°ì˜ í´ë˜ìŠ¤ ê°œìˆ˜
        super(DisturbLabel, self).__init__()
        self.alpha = alpha
        self.C = num_classes
        self.p_c = (1 - ((self.C - 1) / self.C) * (alpha / 100)) # ì‹¤ì œ ë¼ë²¨ì„ ë½‘ì„ í™•ë¥ 
        self.p_i = (1-self.p_c)/(self.C-1) # ë‚˜ë¨¸ì§€
 
    def forward(self, y):
        y_tensor = y.type(torch.LongTensor).view(-1, 1)      
        depth = self.C
        y_one_hot = torch.ones(y_tensor.size()[0], depth) * self.p_i
        y_one_hot.scatter_(1, y_tensor, self.p_c)
        y_one_hot = y_one_hot.view(*(tuple(y.shape) + (-1,))) # create disturbed labels    
        distribution = torch.distributions.OneHotCategorical(y_one_hot) # sample from Multinoulli distribution
        y_disturbed = distribution.sample()
        y_disturbed = y_disturbed.max(dim=1)[1]
        return y_disturbed
```
  
**2. êµë€ ë¼ë²¨ ì¶”ê°€ & í•™ìŠµ ì§„í–‰**  
```Python
disturblabels = DisturbLabel(alpha = 30, num_classes = 10) # êµë€ ë¼ë²¨ ìƒì„±
 
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(resnet.parameters(), lr=1e-3)
loss_ = [] # ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ê¸° ìœ„í•œ loss ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸ 
n = len(trainloader) # ë°°ì¹˜ ê°œìˆ˜
 
for epoch in range(50): 
    running_loss = 0.0
    for data in trainloader:
        inputs, labels = data[0].to(device), data[1].to(device) # ë°°ì¹˜ ë°ì´í„° 
        optimizer.zero_grad()
        
        outputs = resnet(inputs) # ì˜ˆì¸¡ê°’ ì‚°ì¶œ 
        labels = disturblabels(labels).to(device) # ê¸°ì¡´ ë¼ë²¨ -> êµë€ ë¼ë²¨
        
        loss = criterion(outputs, labels) # ì†ì‹¤í•¨ìˆ˜ ê³„ì‚°
        loss.backward() # ì†ì‹¤í•¨ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì—­ì „íŒŒ ì„ ì–¸
        optimizer.step() # ê°€ì¤‘ì¹˜ ìµœì í™”
        running_loss += loss.item()
 
    loss_.append(running_loss / n)    
    print('[%d] loss: %.3f' %(epoch + 1, running_loss / n))
```

# **7. ë¼ë²¨ ìŠ¤ë¬´ë”©(Label Smoothing)**
- ì¼ë°˜ì ì¸ ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” **softmax**ë‚˜ **sigmoid** í•¨ìˆ˜ë¥¼ í†µí•´ì„œ 0 ë˜ëŠ” 1ì˜ ê°’ì„ ì˜ˆì¸¡
- CrossEntropyLossë¥¼ ê³„ì‚°í•  ë•Œ, ì‹¤ì œê°’ì„ 0 or 1ì´ ì•„ë‹Œ 0.2 or 0.8ë¡œ êµ¬ì„±í•´ì„œ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ë°©ì‹
- ë¼ë²¨ 1ì„ ì˜ˆì¸¡ ì‹œ í™•ë¥  ê°’ì´ 0.7ë¡œ ë‚˜íƒ€ë‚˜ë©´ ì›ë˜ëŠ” 1ë¡œ ë‚˜ì˜¤ë„ë¡ í•™ìŠµì´ ì§„í–‰ë¨ 
  - ì´ë•Œ ê¸°ì¤€ì„ 0.8ë¡œ ë‚®ì¶”ë©´ ë³´ë‹¤ ì ê²Œ ì‹¤ì œê°’ê³¼ ê°€ê¹Œì›Œì§€ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•˜ê³ , ì´ë¥¼ í†µí•´ì„œ ê³¼ì í•©ì„ ì™„í™”í•  ìˆ˜ ìˆìŒ
- íŒŒì´í† ì¹˜ì—ì„œ ì œê³µí•˜ëŠ” nn.CrossEntropyLoss() í•¨ìˆ˜ëŠ” ì‹¤ì œ ë¼ë²¨ì˜ ì›-í•« ë²¡í„°ë¥¼ ì…ë ¥ë°›ì„ ìˆ˜ ì—†ë‹¤.
  - ë”°ë¼ì„œ ë¼ë²¨ ìŠ¤ë¬´ë”©ì„ ì ìš©í•˜ë ¤ë©´ One - hot ë²¡í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë³„ë„ë¡œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•´ì•¼ í•¨
- ì½”ë“œ
```Python
class LabelSmoothingLoss(nn.Module):
    def __init__(self, classes, smoothing = 0.0, dim = -1):
    # classes: ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ê°œìˆ˜, smoothing: ìŠ¤ë¬´ë”© ë¹„ìœ¨, dim: ì°¨ì›
        super(LabelSmoothingLoss, self).__init__()
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.cls = classes
        self.dim = dim
 
    def forward(self, pred, target):
        pred = pred.log_softmax(dim=self.dim) # CrossEntropy ë¶€ë¶„ì˜ log softmax ë¯¸ë¦¬ ê³„ì‚°í•˜ê¸°
        with torch.no_grad():
            # true_dist = pred.data.clone()
            true_dist = torch.zeros_like(pred) # ì˜ˆì¸¡ê°’ê³¼ ë™ì¼í•œ í¬ê¸°ì˜ ì˜(zero) í…ì„œ ë§Œë“¤ê¸°
            true_dist.fill_(self.smoothing / (self.cls - 1)) # alpha/(K-1)ì„ ë§Œë“¤ì–´ ì¤Œ(alpha/Kë¡œ í•  ìˆ˜ë„ ìˆìŒ)
            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) # (1-alpha)y + alpha/(K-1)
        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim)) # CrossEntropyLoss ê³„ì‚°
```

```Python
criterion = LabelSmoothingLoss(classes=10, smoothing=0.2)
optimizer = optim.Adam(resnet.parameters(), lr=1e-3)
```








